[
  {
    "objectID": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html",
    "href": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html",
    "title": "",
    "section": "",
    "text": "Table of contents\n- View image with OpenCV: cv2.imshow()\n- The image can be sliced to trim it, which is useful for focusing on specific areas of the image.\n- Read a grayscale image and display it using OpenCV\n- Rotate image with NumPy: np.rot90()\n- We can pass arguments to np.rot90(), defaulting to 1 rotation (90 degrees), 2 rotations (180 degrees), or 3 rotations (270 degrees). The k parameter specifies the number of times to rotate the image by 90 degrees.\n- Tranpose image\n- ‘Plus’ image\n- Flipping image\n- Manually\n- Vertically\n- Horizontally\n- Using built-in functions\n- Using cv2.flip()\n- Increase/Decrease brightness\n- Increase brightness\n- Decrease brightness\n- Background subtraction (or foreground extraction)\n- Manually\n- Using np.subtract()\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Magic command to display inline\n%matplotlib inline\n\n# Helper function to downsample images for display\ndef downsample(img, size=(256, 256)):\n    return cv2.resize(img, size) if img.ndim == 3 else cv2.resize(img, size, interpolation=cv2.INTER_AREA)\npath = 'images'"
  },
  {
    "objectID": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#view-image-with-opencv-cv2.imshow",
    "href": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#view-image-with-opencv-cv2.imshow",
    "title": "",
    "section": "View image with OpenCV: cv2.imshow()",
    "text": "View image with OpenCV: cv2.imshow()\n\nThe image can be sliced to trim it, which is useful for focusing on specific areas of the image.\n\n# Read image\nimage = cv2.imread(path + '/nature.jpg')\nimage_small = downsample(image)\n\n# Show downsampled image and its slicing\nplt.figure(figsize=(4,4))\nplt.imshow(cv2.cvtColor(image_small, cv2.COLOR_BGR2RGB))\nplt.title(\"Image (downsampled)\")\nplt.axis('off')\nplt.show()\n\n# Slicing the image to focus on a specific area, then downsample\nimage_crop = image[50:150, 500:700]\nimage_crop_small = downsample(image_crop)\nplt.figure(figsize=(3,3))\nplt.imshow(cv2.cvtColor(image_crop_small, cv2.COLOR_BGR2RGB))\nplt.title(\"Slicing (downsampled)\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(f'Shape of colorful image: {image.shape}')\nprint(f'Type of colorful image: {type(image)}')\nprint(f'Type of each pixel is: {type(image[0,0,0])}')\n\nShape of colorful image: (400, 650, 3)\nType of colorful image: &lt;class 'numpy.ndarray'&gt;\nType of each pixel is: &lt;class 'numpy.uint8'&gt;\n\n\n\n\nRead a grayscale image and display it using OpenCV\n\nimage_gray = cv2.imread(path + '/nature.jpg', 0)\nimage_gray_small = downsample(image_gray)\nplt.figure(figsize=(4,4))\nplt.imshow(image_gray_small, cmap='gray')\nplt.title(\"Gray (downsampled)\")\nplt.axis('off')\nplt.show()\n\nprint(f'Shape of grayscale image: {image_gray_small.shape}')\n\n\n\n\n\n\n\n\nShape of grayscale image: (256, 256)"
  },
  {
    "objectID": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#rotate-image-with-numpy-np.rot90",
    "href": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#rotate-image-with-numpy-np.rot90",
    "title": "",
    "section": "Rotate image with NumPy: np.rot90()",
    "text": "Rotate image with NumPy: np.rot90()\n\nWe can pass arguments to np.rot90(), defaulting to 1 rotation (90 degrees), 2 rotations (180 degrees), or 3 rotations (270 degrees). The k parameter specifies the number of times to rotate the image by 90 degrees.\n\n# Convert image to RGB for display because OpenCV loads images in BGR format\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_rgb_small = downsample(image_rgb)\nimage_rot_90 = np.rot90(image_rgb_small, k=1)\nimage_rot_180 = np.rot90(image_rgb_small, k=2)\nimage_rot_270 = np.rot90(image_rgb_small, k=3)\nimage_rot_360 = np.rot90(image_rgb_small, k=4)\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\naxes[0, 0].imshow(image_rot_90)\naxes[0, 0].set_title(\"Rotated 90°\")\naxes[0, 1].imshow(image_rot_180)\naxes[0, 1].set_title(\"Rotated 180°\")\naxes[1, 0].imshow(image_rot_270)\naxes[1, 0].set_title(\"Rotated 270°\")\naxes[1, 1].imshow(image_rot_360)\naxes[1, 1].set_title(\"Rotated 360°\")\nfor ax in axes.flat:\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTranpose image\n\nimage_rgb_transposed = image_rgb_small.transpose((1, 0, 2))\n\nfig, axes = plt.subplots(1, 2, figsize=(7, 3), constrained_layout=True)\naxes[0].imshow(image_rgb_small)\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(image_rgb_transposed)\naxes[1].set_title(\"Transposed\")\naxes[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n‘Plus’ image\n\nimage_1 = plt.imread(path + '/colorfull-1.jpg')\nimage_2 = plt.imread(path + '/colorfull-2.jpg')\n\n# Resize image_2 to match image_1's shape\nimage_2_resized = cv2.resize(image_2, (image_1.shape[1], image_1.shape[0]))\n\nimage_plus = image_1 + image_2_resized\nimage_plus_small = downsample(image_plus)\n\nplt.figure(figsize=(4,4))\nplt.imshow(image_plus_small)\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#flipping-image",
    "href": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#flipping-image",
    "title": "",
    "section": "Flipping image",
    "text": "Flipping image\n\nManually\n\nVertically\n\nimg = plt.imread(path + '/nature.jpg')\nimg = img.astype(float)\n\n# Take shapes (height, width, channel)\nheight, width, channels = img.shape\n\n# Transform matrix (flip wrt x axis, or i.e., vertical flip)\ntransform = np.array([[1, 0], [0, -1]])\n\n# Initialize output matrix with same shape as input image\noutput = np.zeros((height, width, channels))\n\n# Iterate over each pixel in the original image\nfor h in range(height):\n    for w in range(width):\n        pixel = img[h, w, :]\n        new_j, new_i = transform@np.array([w, h]) + [0, height - 1]\n        output[new_i, new_j] = pixel\n\noutput = output.astype(np.uint8)\noutput_small = downsample(output)\nplt.imsave(path + '/vertically_flipped.jpg', output_small)\nplt.figure(figsize=(4,4))\nplt.imshow(output_small)\nplt.title(\"Flipped vertically (downsampled)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHorizontally\n\nimg = plt.imread(path + '/nature.jpg')\nimg = img.astype(float)\n\nheight, width, channels = img.shape\ntransform = np.array([[-1, 0], [0, 1]])\noutput = np.zeros((height, width, channels))\nfor h in range(height):\n    for w in range(width):\n        pixel = img[h, w, :]\n        new_j, new_i = transform@np.array([w, h]) + [width - 1, 0]\n        output[new_i, new_j] = pixel\n\noutput = output.astype(np.uint8)\noutput_small = downsample(output)\nplt.imsave(path + '/horizontally_flipped.jpg', output_small)\nplt.figure(figsize=(4,4))\nplt.imshow(output_small)\nplt.title(\"Flipped horizontally (downsampled)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUsing built-in functions\n\nUsing cv2.flip()\nflipCode can be: - 0: flip vertically - 1: flip horizontally - -1: flip both vertically and horizontally\n\nimg = plt.imread(path + '/nature.jpg')\nimg_small = downsample(img)\nfig, ax = plt.subplots(1, 4, figsize=(12, 4), constrained_layout=True)\n\ntitles = [\n    \"Original\",\n    \"Flipped vertically\",\n    \"Flipped horizontally\",\n    \"Flipped vertically & horizontally\"\n]\nimages = [\n    img_small,\n    cv2.flip(img_small, 0),\n    cv2.flip(img_small, 1),\n    cv2.flip(img_small, -1)\n]\n\nfor i in range(4):\n    ax[i].imshow(images[i])\n    ax[i].set_title(titles[i], fontsize=12)\n    ax[i].axis('off')\n\nplt.show()"
  },
  {
    "objectID": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#increasedecrease-brightness",
    "href": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#increasedecrease-brightness",
    "title": "",
    "section": "Increase/Decrease brightness",
    "text": "Increase/Decrease brightness\nBecause OpenCV images contain uint8 pixel values (range [0,255]), so we must convert to float to avoid overflow when adding or subtracting brightness values. After processing, we convert back to uint8.\nWe can implement in 3 ways: loop, np.clip(), and np.where(). - Example of using loop:\nfor i in range(image.shape[0]):\n    for j in range(image.shape[1]):\n        for k in range(image.shape[2]):\n            image[i, j, k] = np.clip(image[i, j, k] + brightness, 0, 255)\n\nExample of using np.clip() in the cell below\nExample of using np.where():\n\nimage = np.where(image + brightness &gt; 255, 255, image + brightness)\n\nIncrease brightness\n\nimage_bright_dec80 = image_rgb_small.copy()\nimage_bright_dec80 = image_bright_dec80.astype(float)\n\nimage_bright_dec80 -= 80\nimage_bright_dec80 = np.clip(image_bright_dec80, 0, 255)\nimage_bright_dec80 = image_bright_dec80.astype(np.uint8)\n\nplt.imsave(path + '/decrease80_clip.jpg', image_bright_dec80)\nplt.figure(figsize=(4,4))\nplt.imshow(image_bright_dec80)\nplt.title(\"Decreased 80 brightness (downsampled)\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDecrease brightness\n\nimage_bright_inc80 = image_rgb_small.copy()\nimage_bright_inc80 = image_bright_inc80.astype(float)\n\nimage_bright_inc80 += 80\nimage_bright_inc80 = np.clip(image_bright_inc80, 0, 255)\nimage_bright_inc80 = image_bright_inc80.astype(np.uint8)\n\nplt.imsave(path + '/increase80_clip.jpg', image_bright_inc80)\nplt.figure(figsize=(4,4))\nplt.imshow(image_bright_inc80)\nplt.title(\"Increased 80 brightness (downsampled)\")\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#background-subtraction-or-foreground-extraction",
    "href": "notebooks/computer_vision/opencv_numpy_image_processing/opencv_numpy_image_processing.html#background-subtraction-or-foreground-extraction",
    "title": "",
    "section": "Background subtraction (or foreground extraction)",
    "text": "Background subtraction (or foreground extraction)\n\nManually\n\nimg = plt.imread(path + '/maxresdefault.jpg')\nrgb8 = (img * 255.0).round().astype(np.uint8) if np.issubdtype(img.dtype, np.floating) else img\nrgb8_small = downsample(rgb8)\n\ntarget = np.array([19, 252, 23], dtype=np.int16)\ntol = 100\ndiff = np.abs(rgb8_small.astype(np.int16) - target)\nis_green = (diff &lt;= tol).all(axis=2)\nextracted = np.where(is_green[..., None], 0, rgb8_small).astype(np.uint8)\n\nfig, ax = plt.subplots(1, 2, figsize=(7, 3), constrained_layout=True)\nax[0].imshow(rgb8_small);      ax[0].set_title('Before');    ax[0].axis('off')\nax[1].imshow(extracted); ax[1].set_title('Extracted'); ax[1].axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nUsing np.subtract()\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Magic command to display inline\n%matplotlib inline\n\n# Load images\nimage_city = plt.imread('images/colorfull-2.jpg')\ndragon_with_background = plt.imread('images/maxresdefault.jpg')\nbackground = plt.imread('images/background.jpg')\n\n# Ensure all images are uint8 and same shape\ndef to_uint8(img):\n    if np.issubdtype(img.dtype, np.floating):\n        img = (img * 255.0).round().astype(np.uint8)\n    return img\n\nimage_city = to_uint8(image_city)\ndragon_with_background = to_uint8(dragon_with_background)\nbackground = to_uint8(background)\n\nimage_city = downsample(image_city)\ndragon_with_background = downsample(dragon_with_background)\nbackground = downsample(background)\n\ndragon_with_background = cv2.resize(dragon_with_background, (image_city.shape[1], image_city.shape[0]))\nbackground = cv2.resize(background, (image_city.shape[1], image_city.shape[0]))\n\n\nprint(image_city.shape, dragon_with_background.shape)\n\n(256, 256, 3) (256, 256, 3)\n\n\n\ndragon_with_background = cv2.resize(dragon_with_background, (image_city.shape[1], image_city.shape[0]))\nbackground = cv2.resize(background, (image_city.shape[1], image_city.shape[0]))\n\n\nprint(image_city.shape, dragon_with_background.shape)\n\n(256, 256, 3) (256, 256, 3)\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(7, 3), constrained_layout=True)\nax[0].imshow(image_city)\nax[0].set_title(\"City\")\nax[0].axis('off')\nax[1].imshow(dragon_with_background)\nax[1].set_title(\"Dragon with background\")\nax[1].axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute absolute difference between dragon_with_background and background\ndiff = cv2.absdiff(dragon_with_background, background)\n\n# Create mask: pixels with large difference are foreground (dragon), small difference are background\nthreshold = 100\nmask = np.any(diff &gt; threshold, axis=2)\n\n# Create output: keep dragon where mask is True, else set to transparent (or black)\ndragon_fg = np.zeros_like(dragon_with_background)\ndragon_fg = np.where(mask[..., None], dragon_with_background, 0)\n\nplt.imshow(dragon_fg)\nplt.title(\"Dragon without background (downsampled)\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Overlay dragon foreground onto city image using mask\noutput = np.where(mask[..., None], dragon_fg, image_city)\nplt.imshow(output)\nplt.title(\"Dragon inside the city (downsampled)\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n# import cv2, matplotlib.pyplot as plt\n\n# # Tạo đối tượng VideoCapture để đọc video từ camera\n# cap = cv2.VideoCapture(0)  # Số 0 thể hiện camera mặc định, nếu có nhiều camera, hãy chọn số thích hợp.\n\n# # Kiểm tra xem camera có mở thành công hay không\n# if not cap.isOpened():\n#     print(\"Không thể mở camera.\")\n#     exit()\n\n# # Đọc khung hình từ camera\n# ret, frame = cap.read()\n\n# # Kiểm tra xem việc đọc khung hình có thành công hay không\n# if not ret:\n#     print(\"Không thể đọc khung hình.\")\n#     exit()\n\n# # Giải phóng tài nguyên\n# cap.release()\n\n# # Hiển thị khung hình inline bằng matplotlib\n# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Chuyển đổi từ BGR sang RGB, vì OpenCV sử dụng BGR\n# plt.axis('off')  # Tắt trục\n# plt.title(\"Camera Frame\")\n# plt.show()"
  },
  {
    "objectID": "notebooks/python_snippets/python_1d_2d_list.html",
    "href": "notebooks/python_snippets/python_1d_2d_list.html",
    "title": "2D List",
    "section": "",
    "text": "Table of contents\n- 1D List\n- Basic\n- Create a 1D list\n- Sum of 1D list\n- Min and Max in 1D list\n- Average of 1D list\n- Sort 1D list\n- Sum of even and odd numbers\n- Advanced\n- Find even/odd numbers and their sums\n- Sum of primes in 1D list\n- Sum of positives and product of negatives\n- Sum after removing duplicates\n- 2D List\n- Basic\n- Sum of 2D list\n- Min and Max in 2D list\n- Average of 2D list\n- Sort sublists in 2D list\n- Advanced\n- Row sums and total sum\n- Column sums and total sum\n- Max value and its indices in 2D list\n- Max value and row containing it"
  },
  {
    "objectID": "notebooks/python_snippets/python_1d_2d_list.html#d-list",
    "href": "notebooks/python_snippets/python_1d_2d_list.html#d-list",
    "title": "2D List",
    "section": "1D List",
    "text": "1D List"
  },
  {
    "objectID": "notebooks/python_snippets/python_1d_2d_list.html#basic",
    "href": "notebooks/python_snippets/python_1d_2d_list.html#basic",
    "title": "2D List",
    "section": "Basic",
    "text": "Basic\n\nCreate a 1D list\n\n# Create a 1d list with 5 elements [1,2,3,4,5] using list comprehension\nmy_list = [i for i in range(1,6)]\nprint(my_list)\n\n[1, 2, 3, 4, 5]\n\n\n\n\nSum of 1D list\n\n# Sum of a 1d list\nmy_list_sum = sum(my_list)\nprint(my_list_sum)\n\n15\n\n\n\n\nMin and Max in 1D list\n\n# Find min/max in a 1d list\nmy_list = [1,2,3,4,5]\nmin_val = min(my_list)\nmax_val = max(my_list)\nprint(min_val, max_val)\n\n1 5\n\n\n\n\nAverage of 1D list\n\n# Average of a 1d list\nmy_list = [1,2,3,4,5]\navg = sum(my_list) / len(my_list)\nprint(avg)\n\n3.0\n\n\n\n\nSort 1D list\n\n# Sort a 1d list ascendingly/descendingly\nmy_list = [5,3,1,4,2]\nascending, descending = sorted(my_list, reverse=False), sorted(my_list, reverse=True)\nprint(ascending, descending)\n\n[1, 2, 3, 4, 5] [5, 4, 3, 2, 1]\n\n\n\n\nSum of even and odd numbers\n\n# Sum of even/odd numbers in a 1d list\nmy_list = [1,2,3,4,5]\neven = sum(list(filter(lambda x: x % 2 == 0, my_list)))\nodd = sum(list(filter(lambda x: x % 2 != 0, my_list)))\nprint(even, odd)\n\n6 9"
  },
  {
    "objectID": "notebooks/python_snippets/python_1d_2d_list.html#advanced",
    "href": "notebooks/python_snippets/python_1d_2d_list.html#advanced",
    "title": "2D List",
    "section": "Advanced",
    "text": "Advanced\n\nFind even/odd numbers and their sums\n\n# Find even/odd numbers and sum of each\nmy_list = [1,2,3,4,5,6,7,8,9,10]\neven_numbers = list(filter(lambda x: x % 2 == 0, my_list))\nsum_even = sum(even_numbers)\nodd_numbers = list(filter(lambda x: x % 2 != 0, my_list))\nsum_odd = sum(odd_numbers)\nprint(even_numbers, sum_even, odd_numbers, sum_odd)\n\n[2, 4, 6, 8, 10] 30 [1, 3, 5, 7, 9] 25\n\n\n\n\nSum of primes in 1D list\n\nimport math\n# Sum of prime numbers in a 1d list\ndef is_prime(n):\n    if n &lt; 2:\n        return False\n    for i in range(2, int(math.sqrt(n)) + 1):\n        if n % i == 0:\n            return False\n    return True\n\nmy_list = [2,3,4,5,6,7,8,9,10]\nprime_numbers = list(filter(is_prime, my_list))\nsum_prime = sum(prime_numbers)\nprint(prime_numbers, sum_prime)\n\n[2, 3, 5, 7] 17\n\n\n\n\nSum of positives and product of negatives\n\n# Sum of positive numbers and product of negatives numbers\nmy_list = [-1,2,-3,4,-5,6]\npositive_numbers = [x for x in my_list if x &gt; 0]\nnegative_numbers = [x for x in my_list if x &lt; 0]\nsum_positive = sum(positive_numbers)\nproduct_negative = 1\nfor neg in negative_numbers:\n    product_negative *= neg\nprint(sum_positive, product_negative)\n\n12 -15\n\n\n\n\nSum after removing duplicates\n\n# Sum of a 1d list after eliminating duplicates\nmy_list = [1, 2, 3, 2, 4, 5, 1, 6, 7, 8, 5]\nunique_numbers = list(set(my_list))\nsum_unique = sum(unique_numbers)\nprint(unique_numbers, sum_unique)\n\n[1, 2, 3, 4, 5, 6, 7, 8] 36"
  },
  {
    "objectID": "notebooks/python_snippets/python_1d_2d_list.html#basic-1",
    "href": "notebooks/python_snippets/python_1d_2d_list.html#basic-1",
    "title": "2D List",
    "section": "Basic",
    "text": "Basic\n\nSum of 2D list\n\n# Sum of elements in a 2d list\nmy_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntotal = sum([sum(sub_list) for sub_list in my_list])\nprint(total)\n\n45\n\n\n\n\nMin and Max in 2D list\n\n# Find max/min in a 2d list\nmy_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmax_value = max([max(sub_list) for sub_list in my_list])\nmin_value = min([min(sub_list) for sub_list in my_list])\nprint(max_value, min_value)\n\n9 1\n\n\n\n\nAverage of 2D list\n\n# Average of a 2d list\nmy_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ntotal = sum([sum(sub_list) for sub_list in my_list])\naverage = total / (len(my_list)*len(my_list[0]))\nprint(average)\n\n5.0\n\n\n\n\nSort sublists in 2D list\n\n# Sort sublists in a 2d list ascending/descending\nmy_list = [[5, 3, 1], [4, 2, 6], [9, 7, 8]]\nascending = [sorted(sublist) for sublist in my_list]\ndescending = [sorted(sublist, reverse=True) for sublist in my_list]\nprint(ascending, descending)\n\n[[1, 3, 5], [2, 4, 6], [7, 8, 9]] [[5, 3, 1], [6, 4, 2], [9, 8, 7]]"
  },
  {
    "objectID": "notebooks/python_snippets/python_1d_2d_list.html#advanced-1",
    "href": "notebooks/python_snippets/python_1d_2d_list.html#advanced-1",
    "title": "2D List",
    "section": "Advanced",
    "text": "Advanced\n\nRow sums and total sum\n\n# Sum of rows and sum of (sum of rows)\nmy_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nrow_sums = [sum(row) for row in my_list]\ntotal_sum = sum(row_sums)\nprint(row_sums, total_sum)\n\n[6, 15, 24] 45\n\n\n\n\nColumn sums and total sum\n\n# Sum of each column and sum of (sum of each column)\nmy_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ncolumn_sums = [sum(row[c] for row in my_list) for c in range(len(my_list[0]))]\ntotal_sum = sum(column_sums)\nprint(column_sums, total_sum)\n\n[12, 15, 18] 45\n\n\n\n\nMax value and its indices in 2D list\n\n# Find max value and its indices in a 2d list\nmy_list = [[1, 2, 3], [4, 9, 6], [7, 8, 5]]\nmax_value = max([max(row) for row in my_list])\nfor idx, row in enumerate(my_list):\n    if max_value in row:\n        max_row, max_col = idx, row.index(max_value)\nprint(max_value, max_row, max_col)\n\n9 1 1\n\n\n\n\nMax value and row containing it\n\n# Find max value and the row containing it in a 2d list\nmy_list = [[1, 2, 3], [4, 9, 6], [7, 8, 5]]\nrows_max = [max(row) for row in my_list]\nmax_value = max(rows_max)\nmax_row = my_list[rows_max.index(max_value)]\nprint(max_value, max_row)\n\n9 [4, 9, 6]"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html",
    "title": "Data Manipulation with Pandas",
    "section": "",
    "text": "import pandas as pd\n\n\ndata = pd.read_csv('data/Pokemon.csv')\n\n\ndata.shape, data.ndim\n\n((800, 13), 2)"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#loading-data-into-pandas-dataframe",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#loading-data-into-pandas-dataframe",
    "title": "Data Manipulation with Pandas",
    "section": "",
    "text": "import pandas as pd\n\n\ndata = pd.read_csv('data/Pokemon.csv')\n\n\ndata.shape, data.ndim\n\n((800, 13), 2)"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#get-headers",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#get-headers",
    "title": "Data Manipulation with Pandas",
    "section": "Get headers",
    "text": "Get headers\n\ndata.columns\n\nIndex(['#', 'Name', 'Type 1', 'Type 2', 'Total', 'HP', 'Attack', 'Defense',\n       'Sp. Atk', 'Sp. Def', 'Speed', 'Generation', 'Legendary'],\n      dtype='object')"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#first-and-last-rows",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#first-and-last-rows",
    "title": "Data Manipulation with Pandas",
    "section": "First and last rows",
    "text": "First and last rows\n\ndata.head(1), data.tail(1)\n\n(   #       Name Type 1  Type 2  Total  HP  Attack  Defense  Sp. Atk  Sp. Def  \\\n 0  1  Bulbasaur  Grass  Poison    318  45      49       49       65       65   \n \n    Speed  Generation  Legendary  \n 0     45           1      False  ,\n        #       Name Type 1 Type 2  Total  HP  Attack  Defense  Sp. Atk  \\\n 799  721  Volcanion   Fire  Water    600  80     110      120      130   \n \n      Sp. Def  Speed  Generation  Legendary  \n 799       90     70           6       True  )"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#high-level-description-of-the-data",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#high-level-description-of-the-data",
    "title": "Data Manipulation with Pandas",
    "section": "High level description of the data",
    "text": "High level description of the data\n\ndata.describe()\n\n\n\n\n\n\n\n\n#\nTotal\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\n\n\n\n\ncount\n800.000000\n800.00000\n800.000000\n800.000000\n800.000000\n800.000000\n800.000000\n800.000000\n800.00000\n\n\nmean\n362.813750\n435.10250\n69.258750\n79.001250\n73.842500\n72.820000\n71.902500\n68.277500\n3.32375\n\n\nstd\n208.343798\n119.96304\n25.534669\n32.457366\n31.183501\n32.722294\n27.828916\n29.060474\n1.66129\n\n\nmin\n1.000000\n180.00000\n1.000000\n5.000000\n5.000000\n10.000000\n20.000000\n5.000000\n1.00000\n\n\n25%\n184.750000\n330.00000\n50.000000\n55.000000\n50.000000\n49.750000\n50.000000\n45.000000\n2.00000\n\n\n50%\n364.500000\n450.00000\n65.000000\n75.000000\n70.000000\n65.000000\n70.000000\n65.000000\n3.00000\n\n\n75%\n539.250000\n515.00000\n80.000000\n100.000000\n90.000000\n95.000000\n90.000000\n90.000000\n5.00000\n\n\nmax\n721.000000\n780.00000\n255.000000\n190.000000\n230.000000\n194.000000\n230.000000\n180.000000\n6.00000"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#get-specific-columns",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#get-specific-columns",
    "title": "Data Manipulation with Pandas",
    "section": "Get specific columns",
    "text": "Get specific columns\n\nmultiple_columns = data[['Name', 'HP', 'Legendary']]\nprint(multiple_columns.head())\n\n                    Name  HP  Legendary\n0              Bulbasaur  45      False\n1                Ivysaur  60      False\n2               Venusaur  80      False\n3  VenusaurMega Venusaur  80      False\n4             Charmander  39      False"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#get-specific-rows",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#get-specific-rows",
    "title": "Data Manipulation with Pandas",
    "section": "Get specific rows",
    "text": "Get specific rows\n\nmiddle_row = data.iloc[0]\nprint(middle_row)\n\n#                     1\nName          Bulbasaur\nType 1            Grass\nType 2           Poison\nTotal               318\nHP                   45\nAttack               49\nDefense              49\nSp. Atk              65\nSp. Def              65\nSpeed                45\nGeneration            1\nLegendary         False\nName: 0, dtype: object"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#get-specific-coordinates-item-in-the-dataframe",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#get-specific-coordinates-item-in-the-dataframe",
    "title": "Data Manipulation with Pandas",
    "section": "Get specific coordinates item in the DataFrame",
    "text": "Get specific coordinates item in the DataFrame\n\nitem = data.iloc[[0, 1, 2], 1]\nprint(item)\n\n0    Bulbasaur\n1      Ivysaur\n2     Venusaur\nName: Name, dtype: object"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#remove-a-column",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#remove-a-column",
    "title": "Data Manipulation with Pandas",
    "section": "Remove a column",
    "text": "Remove a column\n\nHere, if you do not use the columns parameter, you need to specify the axis (0 for rows and 1 for columns).\nAlternatively, you can use the columns parameter to specify the column(s) to drop without needing to specify the axis.\ninplace=True modifies the original DataFrame without needing to reassign it.\n\n\n# Here, 'Total' column is the sum of HP,Attack,Defense, SP. Atk, SP. Def, Speed.\ndata = data.drop(columns='Total')\ndata.head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\n\n\n\n\n0\n1\nBulbasaur\nGrass\nPoison\n45\n49\n49\n65\n65\n45\n1\nFalse\n\n\n1\n2\nIvysaur\nGrass\nPoison\n60\n62\n63\n80\n80\n60\n1\nFalse\n\n\n2\n3\nVenusaur\nGrass\nPoison\n80\n82\n83\n100\n100\n80\n1\nFalse"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#add-a-column",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#add-a-column",
    "title": "Data Manipulation with Pandas",
    "section": "Add a column",
    "text": "Add a column\n\n# So, in order to add the 'Total' column again, we do the summation\ndata['Total'] = data.iloc[:, 4:10].sum(axis=1)\ndata.head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n0\n1\nBulbasaur\nGrass\nPoison\n45\n49\n49\n65\n65\n45\n1\nFalse\n318\n\n\n1\n2\nIvysaur\nGrass\nPoison\n60\n62\n63\n80\n80\n60\n1\nFalse\n405\n\n\n2\n3\nVenusaur\nGrass\nPoison\n80\n82\n83\n100\n100\n80\n1\nFalse\n525"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#iloc-and-loc-in-pandas",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#iloc-and-loc-in-pandas",
    "title": "Data Manipulation with Pandas",
    "section": "iloc and loc in Pandas",
    "text": "iloc and loc in Pandas\n\niloc\n\n# Read single row\ndata.iloc[3]\n\n#                                 3\nName          VenusaurMega Venusaur\nType 1                        Grass\nType 2                       Poison\nHP                               80\nAttack                          100\nDefense                         123\nSp. Atk                         122\nSp. Def                         120\nSpeed                            80\nGeneration                        1\nLegendary                     False\nTotal                           625\nName: 3, dtype: object\n\n\n\n# Read multiple rows\ndata.iloc[[3,6]]\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n3\n3\nVenusaurMega Venusaur\nGrass\nPoison\n80\n100\n123\n122\n120\n80\n1\nFalse\n625\n\n\n6\n6\nCharizard\nFire\nFlying\n78\n84\n78\n109\n85\n100\n1\nFalse\n534\n\n\n\n\n\n\n\n\n# Read a range of rows\ndata.iloc[3:7]\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n3\n3\nVenusaurMega Venusaur\nGrass\nPoison\n80\n100\n123\n122\n120\n80\n1\nFalse\n625\n\n\n4\n4\nCharmander\nFire\nNaN\n39\n52\n43\n60\n50\n65\n1\nFalse\n309\n\n\n5\n5\nCharmeleon\nFire\nNaN\n58\n64\n58\n80\n65\n80\n1\nFalse\n405\n\n\n6\n6\nCharizard\nFire\nFlying\n78\n84\n78\n109\n85\n100\n1\nFalse\n534\n\n\n\n\n\n\n\n\n\nloc\nThe loc property in Pandas is used to access a group of rows and columns by labels or a boolean array. Unlike iloc, which uses integer-based indexing, loc uses the actual labels of the index and columns. This makes it very useful for selecting data based on meaningful row or column names.\n\ndata.loc[data['Legendary'] == True].head()\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n156\n144\nArticuno\nIce\nFlying\n90\n85\n100\n95\n125\n85\n1\nTrue\n580\n\n\n157\n145\nZapdos\nElectric\nFlying\n90\n90\n85\n125\n90\n100\n1\nTrue\n580\n\n\n158\n146\nMoltres\nFire\nFlying\n90\n100\n90\n125\n85\n90\n1\nTrue\n580\n\n\n162\n150\nMewtwo\nPsychic\nNaN\n106\n110\n90\n154\n90\n130\n1\nTrue\n680\n\n\n163\n150\nMewtwoMega Mewtwo X\nPsychic\nFighting\n106\n190\n100\n154\n100\n130\n1\nTrue\n780\n\n\n\n\n\n\n\n\ndata.loc[data['Attack'] == 49, ['Name']]\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nBulbasaur\n\n\n166\nChikorita\n\n\n506\nFinneon"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#filtering-data",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#filtering-data",
    "title": "Data Manipulation with Pandas",
    "section": "Filtering data",
    "text": "Filtering data\nWe can filter with multiple conditions at ease.\n\ndata.loc[(data['Type 1'] == 'Grass') & (data['Type 2'] == 'Poison')].head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n0\n1\nBulbasaur\nGrass\nPoison\n45\n49\n49\n65\n65\n45\n1\nFalse\n318\n\n\n1\n2\nIvysaur\nGrass\nPoison\n60\n62\n63\n80\n80\n60\n1\nFalse\n405\n\n\n2\n3\nVenusaur\nGrass\nPoison\n80\n82\n83\n100\n100\n80\n1\nFalse\n525\n\n\n\n\n\n\n\n\ndata.loc[(data['Type 1'] == 'Grass') | (data['Type 2'] == 'Poison')][0::10].loc[data['HP'] &gt; 50].head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n50\n45\nVileplume\nGrass\nPoison\n75\n80\n85\n110\n90\n50\n1\nFalse\n490\n\n\n101\n94\nGengar\nGhost\nPoison\n60\n65\n60\n130\n75\n110\n1\nFalse\n500\n\n\n197\n182\nBellossom\nGrass\nNaN\n75\n80\n95\n90\n100\n50\n2\nFalse\n490"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#regex-filtering",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#regex-filtering",
    "title": "Data Manipulation with Pandas",
    "section": "Regex Filtering",
    "text": "Regex Filtering\n\nimport re\n\n\nExplicit filtering\n\ndata.loc[data['Name'].str.contains('Mega')].head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n3\n3\nVenusaurMega Venusaur\nGrass\nPoison\n80\n100\n123\n122\n120\n80\n1\nFalse\n625\n\n\n7\n6\nCharizardMega Charizard X\nFire\nDragon\n78\n130\n111\n130\n85\n100\n1\nFalse\n634\n\n\n8\n6\nCharizardMega Charizard Y\nFire\nFlying\n78\n104\n78\n159\n115\n100\n1\nFalse\n634\n\n\n\n\n\n\n\n\n\nRegex filtering, case sensitive by default\n\ndata.loc[data['Type 1'].str.contains('Fire|grass', regex=True)].head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n4\n4\nCharmander\nFire\nNaN\n39\n52\n43\n60\n50\n65\n1\nFalse\n309\n\n\n5\n5\nCharmeleon\nFire\nNaN\n58\n64\n58\n80\n65\n80\n1\nFalse\n405\n\n\n6\n6\nCharizard\nFire\nFlying\n78\n84\n78\n109\n85\n100\n1\nFalse\n534\n\n\n\n\n\n\n\n\n\nRegex filtering, case insensitive\n\ndata.loc[data['Type 1'].str.contains('Fire|grass', flags=re.IGNORECASE, regex=True)].head(10)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n0\n1\nBulbasaur\nGrass\nPoison\n45\n49\n49\n65\n65\n45\n1\nFalse\n318\n\n\n1\n2\nIvysaur\nGrass\nPoison\n60\n62\n63\n80\n80\n60\n1\nFalse\n405\n\n\n2\n3\nVenusaur\nGrass\nPoison\n80\n82\n83\n100\n100\n80\n1\nFalse\n525\n\n\n3\n3\nVenusaurMega Venusaur\nGrass\nPoison\n80\n100\n123\n122\n120\n80\n1\nFalse\n625\n\n\n4\n4\nCharmander\nFire\nNaN\n39\n52\n43\n60\n50\n65\n1\nFalse\n309\n\n\n5\n5\nCharmeleon\nFire\nNaN\n58\n64\n58\n80\n65\n80\n1\nFalse\n405\n\n\n6\n6\nCharizard\nFire\nFlying\n78\n84\n78\n109\n85\n100\n1\nFalse\n534\n\n\n7\n6\nCharizardMega Charizard X\nFire\nDragon\n78\n130\n111\n130\n85\n100\n1\nFalse\n634\n\n\n8\n6\nCharizardMega Charizard Y\nFire\nFlying\n78\n104\n78\n159\n115\n100\n1\nFalse\n634\n\n\n42\n37\nVulpix\nFire\nNaN\n38\n41\n40\n50\n65\n65\n1\nFalse\n299\n\n\n\n\n\n\n\n\n\nUsing regex\n\ndata.loc[data['Name'].str.contains('pi[a-z]*', flags=re.IGNORECASE, regex=True)].head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n13\n10\nCaterpie\nBug\nNaN\n45\n30\n35\n20\n20\n45\n1\nFalse\n195\n\n\n20\n16\nPidgey\nNormal\nFlying\n40\n45\n40\n35\n35\n56\n1\nFalse\n251\n\n\n21\n17\nPidgeotto\nNormal\nFlying\n63\n60\n55\n50\n50\n71\n1\nFalse\n349\n\n\n\n\n\n\n\n\ndata.loc[data['Name'].str.contains('^pi[a-z]*', flags=re.IGNORECASE, regex=True)].head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n20\n16\nPidgey\nNormal\nFlying\n40\n45\n40\n35\n35\n56\n1\nFalse\n251\n\n\n21\n17\nPidgeotto\nNormal\nFlying\n63\n60\n55\n50\n50\n71\n1\nFalse\n349\n\n\n22\n18\nPidgeot\nNormal\nFlying\n83\n80\n75\n70\n70\n101\n1\nFalse\n479\n\n\n\n\n\n\n\n\ndata.loc[data['Name'].str.contains('pi.*', flags=re.IGNORECASE, regex=True)].head(10)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n13\n10\nCaterpie\nBug\nNaN\n45\n30\n35\n20\n20\n45\n1\nFalse\n195\n\n\n20\n16\nPidgey\nNormal\nFlying\n40\n45\n40\n35\n35\n56\n1\nFalse\n251\n\n\n21\n17\nPidgeotto\nNormal\nFlying\n63\n60\n55\n50\n50\n71\n1\nFalse\n349\n\n\n22\n18\nPidgeot\nNormal\nFlying\n83\n80\n75\n70\n70\n101\n1\nFalse\n479\n\n\n23\n18\nPidgeotMega Pidgeot\nNormal\nFlying\n83\n80\n80\n135\n80\n121\n1\nFalse\n579\n\n\n30\n25\nPikachu\nElectric\nNaN\n35\n55\n40\n50\n50\n90\n1\nFalse\n320\n\n\n42\n37\nVulpix\nFire\nNaN\n38\n41\n40\n50\n65\n65\n1\nFalse\n299\n\n\n76\n70\nWeepinbell\nGrass\nPoison\n65\n90\n50\n85\n45\n55\n1\nFalse\n390\n\n\n84\n78\nRapidash\nFire\nNaN\n65\n100\n70\n80\n80\n105\n1\nFalse\n500\n\n\n136\n127\nPinsir\nBug\nNaN\n65\n125\n100\n55\n70\n85\n1\nFalse\n500"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#sorting",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#sorting",
    "title": "Data Manipulation with Pandas",
    "section": "Sorting",
    "text": "Sorting\n\nSort by 1 column, ascending\n\ndata.sort_values('Type 1').head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n600\n540\nSewaddle\nBug\nGrass\n45\n53\n70\n40\n60\n42\n5\nFalse\n310\n\n\n136\n127\nPinsir\nBug\nNaN\n65\n125\n100\n55\n70\n85\n1\nFalse\n500\n\n\n457\n412\nBurmy\nBug\nNaN\n40\n29\n45\n29\n45\n36\n4\nFalse\n224\n\n\n\n\n\n\n\n\nSort by 1 column, descending\n\ndata.sort_values('Type 1', ascending=False).head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n371\n339\nBarboach\nWater\nGround\n50\n48\n43\n46\n41\n60\n3\nFalse\n288\n\n\n97\n90\nShellder\nWater\nNaN\n30\n65\n100\n45\n25\n40\n1\nFalse\n305\n\n\n240\n222\nCorsola\nWater\nRock\n55\n55\n85\n65\n85\n35\n2\nFalse\n380\n\n\n\n\n\n\n\n\n\nSort by multiple columns, 1 = ascending, 0 = descending\n\ndata.sort_values(['Type 1', 'HP'], ascending=[1, 0]).head(3)\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n520\n469\nYanmega\nBug\nFlying\n86\n76\n86\n116\n56\n95\n4\nFalse\n515\n\n\n698\n637\nVolcarona\nBug\nFire\n85\n60\n65\n135\n105\n100\n5\nFalse\n550\n\n\n231\n214\nHeracross\nBug\nFighting\n80\n125\n75\n40\n95\n85\n2\nFalse\n500"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#reset-index",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#reset-index",
    "title": "Data Manipulation with Pandas",
    "section": "Reset index",
    "text": "Reset index\nIn the cell below, you can see the index is not in order (the leftmost column). We can reset it with the reset_index() method. By default, it adds the old index as a new column. If you do not want that, use the drop=True parameter.\n\nnew_data = data.loc[(data['Type 1'] == 'Grass') | (data['Type 2'] == 'Poison')][0::10].loc[data['HP'] &gt; 50].head(3)\nnew_data\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n50\n45\nVileplume\nGrass\nPoison\n75\n80\n85\n110\n90\n50\n1\nFalse\n490\n\n\n101\n94\nGengar\nGhost\nPoison\n60\n65\n60\n130\n75\n110\n1\nFalse\n500\n\n\n197\n182\nBellossom\nGrass\nNaN\n75\n80\n95\n90\n100\n50\n2\nFalse\n490\n\n\n\n\n\n\n\n\nnew_data.reset_index(drop=True, inplace=True)\nnew_data\n\n\n\n\n\n\n\n\n#\nName\nType 1\nType 2\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nLegendary\nTotal\n\n\n\n\n0\n45\nVileplume\nGrass\nPoison\n75\n80\n85\n110\n90\n50\n1\nFalse\n490\n\n\n1\n94\nGengar\nGhost\nPoison\n60\n65\n60\n130\n75\n110\n1\nFalse\n500\n\n\n2\n182\nBellossom\nGrass\nNaN\n75\n80\n95\n90\n100\n50\n2\nFalse\n490"
  },
  {
    "objectID": "notebooks/pandas/basic_pandas/basic_pandas.html#group-byaggregate-using-aggregate-function",
    "href": "notebooks/pandas/basic_pandas/basic_pandas.html#group-byaggregate-using-aggregate-function",
    "title": "Data Manipulation with Pandas",
    "section": "Group by/Aggregate using aggregate function",
    "text": "Group by/Aggregate using aggregate function\n\nGroup by 1 column\n\n# Select only numeric columns for aggregation\nnumeric_cols = data.select_dtypes(include='number').columns\ndata.groupby(['Type 1'])[numeric_cols].mean().sort_values('Defense', ascending=False).head(10)\n\n\n\n\n\n\n\n\n#\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nTotal\n\n\nType 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nSteel\n442.851852\n65.222222\n92.703704\n126.370370\n67.518519\n80.629630\n55.259259\n3.851852\n487.703704\n\n\nRock\n392.727273\n65.363636\n92.863636\n100.795455\n63.340909\n75.477273\n55.909091\n3.454545\n453.750000\n\n\nDragon\n474.375000\n83.312500\n112.125000\n86.375000\n96.843750\n88.843750\n83.031250\n3.875000\n550.531250\n\n\nGround\n356.281250\n73.781250\n95.750000\n84.843750\n56.468750\n62.750000\n63.906250\n3.156250\n437.500000\n\n\nGhost\n486.500000\n64.437500\n73.781250\n81.187500\n79.343750\n76.468750\n64.343750\n4.187500\n439.562500\n\n\nWater\n303.089286\n72.062500\n74.151786\n72.946429\n74.812500\n70.517857\n65.964286\n2.857143\n430.455357\n\n\nIce\n423.541667\n72.000000\n72.750000\n71.416667\n77.541667\n76.291667\n63.458333\n3.541667\n433.458333\n\n\nGrass\n344.871429\n67.271429\n73.214286\n70.800000\n77.500000\n70.428571\n61.928571\n3.357143\n421.142857\n\n\nBug\n334.492754\n56.884058\n70.971014\n70.724638\n53.869565\n64.797101\n61.681159\n3.217391\n378.927536\n\n\nDark\n461.354839\n66.806452\n88.387097\n70.225806\n74.645161\n69.516129\n76.161290\n4.032258\n445.741935\n\n\n\n\n\n\n\n\n\nGroup by hierarchical columns\n\nnumeric_cols = data.select_dtypes(include='number').columns\ndata.groupby(['Type 1', 'Type 2'])[numeric_cols].count()\n\n\n\n\n\n\n\n\n\n#\nHP\nAttack\nDefense\nSp. Atk\nSp. Def\nSpeed\nGeneration\nTotal\n\n\nType 1\nType 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nBug\nElectric\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\nFighting\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\nFire\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\nFlying\n14\n14\n14\n14\n14\n14\n14\n14\n14\n\n\nGhost\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWater\nIce\n3\n3\n3\n3\n3\n3\n3\n3\n3\n\n\nPoison\n3\n3\n3\n3\n3\n3\n3\n3\n3\n\n\nPsychic\n5\n5\n5\n5\n5\n5\n5\n5\n5\n\n\nRock\n4\n4\n4\n4\n4\n4\n4\n4\n4\n\n\nSteel\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n136 rows × 9 columns"
  },
  {
    "objectID": "notebooks/optimization/optimization_using_newton_method/optimization_using_newton_method.html",
    "href": "notebooks/optimization/optimization_using_newton_method/optimization_using_newton_method.html",
    "title": "",
    "section": "",
    "text": "Table of contents\n- Function in One Variable\n- Function in Two Variables"
  },
  {
    "objectID": "notebooks/optimization/optimization_using_newton_method/optimization_using_newton_method.html#function-in-one-variable",
    "href": "notebooks/optimization/optimization_using_newton_method/optimization_using_newton_method.html#function-in-one-variable",
    "title": "",
    "section": "Function in One Variable",
    "text": "Function in One Variable\nYou will use Newton’s method to optimize a function \\(f\\left(x\\right)\\). Aiming to find a point, where the derivative equals to zero, you need to start from some initial point \\(x_0\\), calculate first and second derivatives (\\(f'(x_0)\\) and \\(f''(x_0)\\)) and step to the next point using the expression:\n\\[x_1 = x_0 - \\frac{f'(x_0)}{f''(x_0)},\\tag{1}\\]\nRepeat the process iteratively. Number of iterations \\(n\\) is usually also a parameter.\nLet’s optimize function \\(f\\left(x\\right)=e^x - \\log(x)\\) (defined for \\(x&gt;0\\)) using Newton’s method. To implement it in the code, define function \\(f\\left(x\\right)=e^x - \\log(x)\\), its first and second derivatives \\(f'(x)=e^x - \\frac{1}{x}\\), \\(f''(x)=e^x + \\frac{1}{x^2}\\):\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax, jax.numpy as jnp\n\ndef f_example_1(x):\n    return jnp.exp(x) - jnp.log(x)\n\ndfdx_example_1 = jax.grad(f_example_1)\nd2fdx2_example_1 = jax.grad(dfdx_example_1)\n\nx_0 = 1.6\nprint(f\"f({x_0}) = {f_example_1(x_0)}\")\nprint(f\"f'({x_0}) = {dfdx_example_1(x_0)}\")\nprint(f\"f''({x_0}) = {d2fdx2_example_1(x_0)}\")\n\nf(1.6) = 4.483028888702393\nf'(1.6) = 4.328032493591309\nf''(1.6) = 5.343657493591309\n\n\nImplement Newton’s method described above.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax, jax.numpy as jnp\n\ndef newton_method(dfdx, d2fdx2, x, num_iterations=100):\n    for iteration in range(num_iterations):\n        x = x - dfdx(x) /d2fdx2(x)\n        print(x)\n    return x\n\n\nnum_iterations_example_1 = 25; x_initial = 1.6\nnewtons_example_1 = newton_method(dfdx_example_1, d2fdx2_example_1, x_initial, num_iterations_example_1)\nprint(\"Newton's method result: x_min =\", newtons_example_1)\n\n0.7900617721793732\n0.5436324685389214\n0.5665913613835818\n0.567143002403454\n0.5671432904097056\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\n0.5671432904097838\nNewton's method result: x_min = 0.5671432904097838\n\n\nLet’s compare with Gradient Descent method.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax, jax.numpy as jnp\n\ndef gradient_descent(dfdx, x, learning_rate=0.1, num_iterations=100):\n    for iteration in range(num_iterations):\n        x = x - learning_rate * dfdx(x)\n        print(x)\n    return x\n\n\nnum_iterations = 35; learning_rate = 0.25; x_initial = 1.6\ngd_example_1 = gradient_descent(dfdx_example_1, x_initial, learning_rate, num_iterations)\nprint(\"Gradient descent result: x_min =\", gd_example_1) \n\n0.5179919\n0.5809616\n0.56434345\n0.56776285\n0.5670086\n0.56717265\n0.5671369\n0.5671447\n0.56714296\n0.5671434\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\n0.56714326\n0.5671433\nGradient descent result: x_min = 0.5671433"
  },
  {
    "objectID": "notebooks/optimization/optimization_using_newton_method/optimization_using_newton_method.html#function-in-two-variables",
    "href": "notebooks/optimization/optimization_using_newton_method/optimization_using_newton_method.html#function-in-two-variables",
    "title": "",
    "section": "Function in Two Variables",
    "text": "Function in Two Variables\nIn case of a function in two variables, Newton’s method will require even more computations. Starting from the intial point \\((x_0, y_0)\\), the step to the next point shoud be done using the expression:\n\\[\\begin{bmatrix}x_1 \\\\ y_1\\end{bmatrix} = \\begin{bmatrix}x_0 \\\\ y_0\\end{bmatrix} -\nH^{-1}\\left(x_0, y_0\\right)\\nabla f\\left(x_0, y_0\\right),\\tag{2}\\]\nwhere \\(H^{-1}\\left(x_0, y_0\\right)\\) is an inverse of a Hessian matrix at point \\((x_0, y_0)\\) and \\(\\nabla f\\left(x_0, y_0\\right)\\) is the gradient at that point.\nLet’s implement that in the code. Define the function \\(f(x, y)\\) like in the videos, its gradient and Hessian:\n\\[\\begin{align}\nf\\left(x, y\\right) &= x^4 + 0.8 y^4 + 4x^2 + 2y^2 - xy - 0.2x^2y,\\\\\n\\nabla f\\left(x, y\\right) &= \\begin{bmatrix}4x^3 + 8x - y - 0.4xy \\\\ 3.2y^3 + 4y - x - 0.2x^2\\end{bmatrix}, \\\\\nH\\left(x, y\\right) &= \\begin{bmatrix}12x^2 + 8 - 0.4y && -1 - 0.4x \\\\ -1 - 0.4x && 9.6y^2 + 4\\end{bmatrix}.\n\\end{align}\\]\n\nimport jax\nimport jax.numpy as jnp\n\ndef f_example_2(xy):\n    x, y = xy[0], xy[1]\n    return x**4 + 0.8*y**4 + 4*x**2 + 2*y**2 - x*y - 0.2*x**2*y\n\ngrad_f_example_2 = jax.grad(f_example_2)\nhessian_f_example_2 = jax.hessian(f_example_2)\n\n# Example usage:\nxy_0 = jnp.array([4.0, 4.0])\nprint(\"f(4,4) =\", f_example_2(xy_0))\nprint(\"grad f(4,4) =\", grad_f_example_2(xy_0))\nprint(\"Hessian f(4,4) =\", hessian_f_example_2(xy_0))\n\nf(4,4) = 528.0\ngrad f(4,4) = [277.6 213.6]\nHessian f(4,4) = [[198.4  -2.6]\n [ -2.6 157.6]]\n\n\n\ndef newton_method_2(hessian_f, grad_f, x_y, num_iterations=100):\n    for iteration in range(num_iterations):\n        x_y = x_y - np.linalg.inv(hessian_f(x_y)) @ grad_f(x_y)\n        print(x_y.T)\n    return x_y\n\nnum_iterations_example_2 = 25\nx_y_initial = jnp.array([4.0, 4.0])\nnewtons_example_2 = newton_method_2(hessian_f_example_2, grad_f_example_2,\n                                    x_y_initial, num_iterations=num_iterations_example_2)\nprint(\"Newton's method result: x_min, y_min =\", newtons_example_2)\n\n[2.5827386 2.6212888]\n[1.5922568 1.6748161]\n[0.870589 1.001821]\n[0.33519423 0.49397618]\n[0.04123583 0.12545902]\n[0.00019466 0.00301028]\n[-2.4869223e-08  3.5157427e-08]\n[-1.7763568e-15  0.0000000e+00]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\n[0. 0.]\nNewton's method result: x_min, y_min = [0. 0.]\n\n\nCompare with Gradient Descent method.\n\ndef gradient_descent_2(grad_f, x_y, learning_rate=0.1, num_iterations=100):\n    for iteration in range(num_iterations):\n        x_y = x_y - learning_rate * grad_f(x_y)\n        print(x_y.T)\n    return x_y\n\n\nnum_iterations_2 = 300; learning_rate_2 = 0.01; x_y_initial = np.array([4., 4.])\n# num_iterations_2 = 300; learning_rate_2 = 0.03; x_y_initial = np.array([[4], [4]])\ngd_example_2 = gradient_descent_2(grad_f_example_2, x_y_initial, learning_rate_2, num_iterations_2)\nprint(\"Gradient descent result: x_min, y_min =\", gd_example_2) \n\n[1.224     1.8640001]\n[1.0804955 1.5974296]\n[0.9664763 1.416231 ]\n[0.872685  1.2802172]\n[0.7935566 1.1721154]\n[0.72552466 1.0828958 ]\n[0.6661781 1.0072521]\n[0.6138146  0.94181013]\n[0.5671893 0.8842969]\n[0.5253647  0.83311224]\n[0.4876172  0.78708965]\n[0.45337626 0.74535424]\n[0.42218375 0.70723426]\n[0.39366573 0.67220336]\n[0.3675127  0.63984215]\n[0.34346518 0.6098113 ]\n[0.32130316 0.58183277]\n[0.3008382 0.555676 ]\n[0.2819075 0.5311478]\n[0.26436916 0.5080848 ]\n[0.24809869 0.48634768]\n[0.23298606 0.46581665]\n[0.21893358 0.446388  ]\n[0.20585394 0.42797133]\n[0.19366881 0.41048738]\n[0.18230762 0.39386624]\n[0.17170653 0.37804592]\n[0.16180763 0.36297116]\n[0.1525582 0.3485925]\n[0.14391015 0.3348654 ]\n[0.13581954 0.3217497 ]\n[0.12824605 0.30920893]\n[0.12115271 0.2972099 ]\n[0.11450549 0.28572226]\n[0.10827309 0.27471823]\n[0.10242663 0.26417223]\n[0.09693947 0.25406063]\n[0.091787   0.24436162]\n[0.08694644 0.23505495]\n[0.08239673 0.22612175]\n[0.07811836 0.21754445]\n[0.07409324 0.20930661]\n[0.07030462 0.20139283]\n[0.06673691 0.19378866]\n[0.06337569 0.1864805 ]\n[0.06020753 0.17945556]\n[0.05721997 0.17270173]\n[0.05440142 0.16620758]\n[0.05174111 0.15996228]\n[0.04922901 0.15395558]\n[0.04685579 0.14817773]\n[0.04461276 0.14261946]\n[0.04249183 0.13727196]\n[0.04048547 0.13212684]\n[0.03858665 0.12717609]\n[0.03678881 0.12241207]\n[0.03508585 0.11782748]\n[0.03347206 0.11341536]\n[0.03194214 0.10916902]\n[0.0304911  0.10508209]\n[0.02911432 0.10114844]\n[0.02780745 0.09736223]\n[0.02656644 0.09371783]\n[0.02538751 0.09020985]\n[0.02426712 0.08683313]\n[0.02320194 0.0835827 ]\n[0.02218887 0.08045381]\n[0.021225   0.07744186]\n[0.02030761 0.07454248]\n[0.01943415 0.07175142]\n[0.01860221 0.06906464]\n[0.01780957 0.06647822]\n[0.01705409 0.06398842]\n[0.01633382 0.06159163]\n[0.01564688 0.05928436]\n[0.01499153 0.05706327]\n[0.01436612 0.05492516]\n[0.01376912 0.05286692]\n[0.01319907 0.05088559]\n[0.0126546  0.04897829]\n[0.01213441 0.04714226]\n[0.0116373  0.04537486]\n[0.01116211 0.04367352]\n[0.01070777 0.04203578]\n[0.01027326 0.04045928]\n[0.00985761 0.03894174]\n[0.00945992 0.03748095]\n[0.00907932 0.03607481]\n[0.008715   0.03472127]\n[0.0083662  0.03341838]\n[0.00803218 0.03216426]\n[0.00771226 0.03095707]\n[0.00740579 0.02979508]\n[0.00711214 0.0286766 ]\n[0.00683074 0.0276    ]\n[0.00656102 0.02656373]\n[0.00630246 0.02556628]\n[0.00605456 0.0246062 ]\n[0.00581685 0.02368209]\n[0.00558886 0.02279262]\n[0.00537018 0.02193649]\n[0.0051604  0.02111245]\n[0.00495912 0.02031931]\n[0.00476598 0.01955591]\n[0.00458063 0.01882114]\n[0.00440273 0.01811393]\n[0.00423197 0.01743324]\n[0.00406804 0.0167781 ]\n[0.00391064 0.01614754]\n[0.00375952 0.01554064]\n[0.0036144  0.01495652]\n[0.00347502 0.01439432]\n[0.00334116 0.01385323]\n[0.00321259 0.01333245]\n[0.00308907 0.01283122]\n[0.00297042 0.01234881]\n[0.00285642 0.01188452]\n[0.00274688 0.01143767]\n[0.00264164 0.0110076 ]\n[0.0025405  0.01059368]\n[0.0024433  0.01019531]\n[0.00234989 0.00981191]\n[0.00226011 0.00944292]\n[0.00217381 0.00908778]\n[0.00209086 0.008746  ]\n[0.00201113 0.00841705]\n[0.00193448 0.00810047]\n[0.00186079 0.00779579]\n[0.00178994 0.00750255]\n[0.00172182 0.00722034]\n[0.00165633 0.00694874]\n[0.00159336 0.00668735]\n[0.0015328  0.00643579]\n[0.00147458 0.00619368]\n[0.00141858 0.00596067]\n[0.00136474 0.00573643]\n[0.00131295 0.00552062]\n[0.00126315 0.00531292]\n[0.00121526 0.00511303]\n[0.00116919 0.00492066]\n[0.00112489 0.00473553]\n[0.00108227 0.00455735]\n[0.00104128 0.00438588]\n[0.00100186 0.00422086]\n[0.00096393 0.00406204]\n[0.00092746 0.0039092 ]\n[0.00089237 0.00376211]\n[0.00085861 0.00362055]\n[0.00082614 0.00348431]\n[0.0007949 0.0033532]\n[0.00076485 0.00322702]\n[0.00073595 0.00310559]\n[0.00070813 0.00298872]\n[0.00068138 0.00287626]\n[0.00065564 0.00276802]\n[0.00063088 0.00266386]\n[0.00060705 0.00256361]\n[0.00058413 0.00246714]\n[0.00056208 0.00237429]\n[0.00054086 0.00228494]\n[0.00052044 0.00219895]\n[0.0005008 0.0021162]\n[0.0004819  0.00203656]\n[0.00046372 0.00195992]\n[0.00044623 0.00188616]\n[0.00042939 0.00181517]\n[0.0004132  0.00174686]\n[0.00039761 0.00168112]\n[0.00038262 0.00161785]\n[0.00036819 0.00155696]\n[0.00035431 0.00149837]\n[0.00034095 0.00144198]\n[0.00032809 0.00138771]\n[0.00031572 0.00133548]\n[0.00030382 0.00128522]\n[0.00029237 0.00123685]\n[0.00028135 0.0011903 ]\n[0.00027075 0.0011455 ]\n[0.00026054 0.00110239]\n[0.00025073 0.0010609 ]\n[0.00024128 0.00102097]\n[0.00023219 0.00098254]\n[0.00022344 0.00094556]\n[0.00021502 0.00090997]\n[0.00020692 0.00087573]\n[0.00019912 0.00084277]\n[0.00019162 0.00081105]\n[0.0001844  0.00078052]\n[0.00017746 0.00075114]\n[0.00017077 0.00072287]\n[0.00016434 0.00069567]\n[0.00015815 0.00066948]\n[0.00015219 0.00064428]\n[0.00014646 0.00062004]\n[0.00014094 0.0005967 ]\n[0.00013564 0.00057424]\n[0.00013053 0.00055263]\n[0.00012561 0.00053183]\n[0.00012088 0.00051181]\n[0.00011633 0.00049255]\n[0.00011195 0.00047401]\n[0.00010773 0.00045617]\n[0.00010368 0.000439  ]\n[9.9772391e-05 4.2247464e-04]\n[9.6015516e-05 4.0657338e-04]\n[9.2400165e-05 3.9127062e-04]\n[8.8921006e-05 3.7654382e-04]\n[8.557290e-05 3.623713e-04]\n[8.235090e-05 3.487322e-04]\n[7.9250269e-05 3.3560643e-04]\n[7.626642e-05 3.229747e-04]\n[7.3394949e-05 3.1081837e-04]\n[7.063163e-05 2.991196e-04]\n[6.7972374e-05 2.8786113e-04]\n[6.5413275e-05 2.7702641e-04]\n[6.295055e-05 2.665995e-04]\n[6.0580569e-05 2.5656502e-04]\n[5.8299836e-05 2.4690822e-04]\n[5.610499e-05 2.376149e-04]\n[5.3992793e-05 2.2867137e-04]\n[5.1960134e-05 2.2006445e-04]\n[5.0004015e-05 2.1178149e-04]\n[4.8121550e-05 2.0381027e-04]\n[4.6309968e-05 1.9613908e-04]\n[4.4566597e-05 1.8875662e-04]\n[4.2888871e-05 1.8165202e-04]\n[4.1274314e-05 1.7481484e-04]\n[3.9720548e-05 1.6823498e-04]\n[3.8225280e-05 1.6190279e-04]\n[3.6786310e-05 1.5580893e-04]\n[3.5401517e-05 1.4994443e-04]\n[3.4068860e-05 1.4430068e-04]\n[3.2786378e-05 1.3886933e-04]\n[3.1552179e-05 1.3364243e-04]\n[3.0364447e-05 1.2861226e-04]\n[2.9221428e-05 1.2377142e-04]\n[2.81214416e-05 1.19112774e-04]\n[2.7062868e-05 1.1462948e-04]\n[2.6044147e-05 1.1031493e-04]\n[2.5063775e-05 1.0616278e-04]\n[2.4120311e-05 1.0216691e-04]\n[2.3212364e-05 9.8321434e-05]\n[2.2338598e-05 9.4620700e-05]\n[2.1497726e-05 9.1059257e-05]\n[2.0688509e-05 8.7631866e-05]\n[1.9909754e-05 8.4333478e-05]\n[1.9160316e-05 8.1159240e-05]\n[1.8439088e-05 7.8104473e-05]\n[1.7745011e-05 7.5164688e-05]\n[1.7077062e-05 7.2335548e-05]\n[1.6434256e-05 6.9612899e-05]\n[1.5815649e-05 6.6992725e-05]\n[1.5220328e-05 6.4471176e-05]\n[1.4647418e-05 6.2044535e-05]\n[1.4096073e-05 5.9709229e-05]\n[1.3565483e-05 5.7461821e-05]\n[1.3054865e-05 5.5299002e-05]\n[1.2563469e-05 5.3217591e-05]\n[1.2090570e-05 5.1214523e-05]\n[1.1635472e-05 4.9286849e-05]\n[1.1197505e-05 4.7431731e-05]\n[1.0776024e-05 4.5646437e-05]\n[1.0370409e-05 4.3928339e-05]\n[9.980061e-06 4.227491e-05]\n[9.6044068e-06 4.0683713e-05]\n[9.2428927e-06 3.9152408e-05]\n[8.894986e-06 3.767874e-05]\n[8.560176e-06 3.626054e-05]\n[8.237968e-06 3.489572e-05]\n[7.927889e-06 3.358227e-05]\n[7.6294818e-06 3.2318258e-05]\n[7.3423071e-06 3.1101823e-05]\n[7.0659416e-06 2.9931172e-05]\n[6.7999790e-06 2.8804585e-05]\n[6.5440272e-06 2.7720402e-05]\n[6.2977097e-06 2.6677026e-05]\n[6.0606640e-06 2.5672922e-05]\n[5.8325409e-06 2.4706611e-05]\n[5.6130043e-06 2.3776673e-05]\n[5.4017314e-06 2.2881735e-05]\n[5.1984107e-06 2.2020484e-05]\n[5.0027429e-06 2.1191649e-05]\n[4.814440e-06 2.039401e-05]\n[4.6332257e-06 1.9626395e-05]\n[4.4588319e-06 1.8887671e-05]\n[4.2910024e-06 1.8176752e-05]\n[4.1294902e-06 1.7492592e-05]\n[3.9740571e-06 1.6834183e-05]\n[3.8244748e-06 1.6200556e-05]\n[3.6805227e-06 1.5590778e-05]\n[3.5419889e-06 1.5003952e-05]\n[3.4086695e-06 1.4439214e-05]\n[3.2803682e-06 1.3895732e-05]\n[3.1568964e-06 1.3372706e-05]\n[3.0380718e-06 1.2869367e-05]\n[2.9237199e-06 1.2384973e-05]\n[2.8136722e-06 1.1918812e-05]\n[2.7077667e-06 1.1470196e-05]\n[2.6058474e-06 1.1038466e-05]\n[2.5077643e-06 1.0622986e-05]\nGradient descent result: x_min, y_min = [2.5077643e-06 1.0622986e-05]"
  },
  {
    "objectID": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html",
    "href": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html",
    "title": "",
    "section": "",
    "text": "Table of contents\n- Packages\n- Import datasets from Kaggle\n- Data Exploration\n- Print first few rows, shape, and data types\n- Use describe() to get summary statistics\n- Data Preparation\n- Manipulate categorical variables\n- Eliminate disturbing values\n- Eliminate values that &gt; 99% of the rest\n- Data for Training and Testing\n- Modeling\n- Linear Regression Model\n- Training"
  },
  {
    "objectID": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#packages",
    "href": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#packages",
    "title": "",
    "section": "Packages",
    "text": "Packages\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#import-datasets-from-kaggle",
    "href": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#import-datasets-from-kaggle",
    "title": "",
    "section": "Import datasets from Kaggle",
    "text": "Import datasets from Kaggle\nThis dataset is available on Kaggle at Diamond Price Prediction.\n\ndiamond_df = pd.read_csv('data/diamonds.csv')\n\n\ndiamond_df.head(), diamond_df.shape\n\n(   Unnamed: 0  carat      cut color clarity  depth  table  price     x     y  \\\n 0           1   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98   \n 1           2   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84   \n 2           3   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07   \n 3           4   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23   \n 4           5   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35   \n \n       z  \n 0  2.43  \n 1  2.31  \n 2  2.31  \n 3  2.63  \n 4  2.75  ,\n (53940, 11))\n\n\nEliminate the leftmost column, which is just an index.\n\ndiamond_df = diamond_df.drop(['Unnamed: 0'], axis=1)"
  },
  {
    "objectID": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#data-exploration",
    "href": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#data-exploration",
    "title": "",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nPrint first few rows, shape, and data types\n\ndiamond_df.head(), diamond_df.shape, diamond_df.dtypes\n\n(   carat      cut color clarity  depth  table  price     x     y     z\n 0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n 1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n 2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n 3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n 4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75,\n (53940, 10),\n carat      float64\n cut         object\n color       object\n clarity     object\n depth      float64\n table      float64\n price        int64\n x          float64\n y          float64\n z          float64\n dtype: object)\n\n\n\n\nUse describe() to get summary statistics\n\ndiamond_df.describe()\n\n\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncount\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n\n\nmean\n0.797940\n61.749405\n57.457184\n3932.799722\n5.731157\n5.734526\n3.538734\n\n\nstd\n0.474011\n1.432621\n2.234491\n3989.439738\n1.121761\n1.142135\n0.705699\n\n\nmin\n0.200000\n43.000000\n43.000000\n326.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.400000\n61.000000\n56.000000\n950.000000\n4.710000\n4.720000\n2.910000\n\n\n50%\n0.700000\n61.800000\n57.000000\n2401.000000\n5.700000\n5.710000\n3.530000\n\n\n75%\n1.040000\n62.500000\n59.000000\n5324.250000\n6.540000\n6.540000\n4.040000\n\n\nmax\n5.010000\n79.000000\n95.000000\n18823.000000\n10.740000\n58.900000\n31.800000\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Print 7 histograms, with each title describe shortly the distribution style of the data (e.g. right-skewed, clustered,...)\ndiamond_df.hist(figsize=(20, 15));\n\n\n\n\n\n\n\n\n\n# The scatter matrix is a great way to visualize the relationships between multiple variables in a dataset.\n\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(diamond_df, figsize=(20, 15), diagonal='kde');"
  },
  {
    "objectID": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#data-preparation",
    "href": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#data-preparation",
    "title": "",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nManipulate categorical variables\n\ndiamond_df['cut'].unique(), diamond_df['color'].unique(), diamond_df['clarity'].unique()\n\n(array(['Ideal', 'Premium', 'Good', 'Very Good', 'Fair'], dtype=object),\n array(['E', 'I', 'J', 'H', 'F', 'G', 'D'], dtype=object),\n array(['SI2', 'SI1', 'VS1', 'VS2', 'VVS2', 'VVS1', 'I1', 'IF'],\n       dtype=object))\n\n\n\ncolor_mapping = {'J': 0, 'I': 1, 'H': 2, 'G': 3, 'F': 4, 'E': 5, 'D': 6}\ndiamond_df.color = diamond_df.color.map(color_mapping)\n\n\nclarity_mapping = {'I1': 0, 'SI2': 1, 'SI1': 2, 'VS2': 3, 'VS1': 4, 'VVS2': 5, 'VVS1': 6, 'IF': 7}\ndiamond_df.clarity = diamond_df.clarity.map(clarity_mapping)\n\n\ncut_mapping = {'Fair': 0, 'Good': 1, 'Very Good': 2, 'Premium': 3, 'Ideal': 4}\ndiamond_df.cut = diamond_df.cut.map(cut_mapping)\n\n\ndiamond_df.describe()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncount\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n53940.000000\n\n\nmean\n0.797940\n2.904097\n3.405803\n3.051020\n61.749405\n57.457184\n3932.799722\n5.731157\n5.734526\n3.538734\n\n\nstd\n0.474011\n1.116600\n1.701105\n1.647136\n1.432621\n2.234491\n3989.439738\n1.121761\n1.142135\n0.705699\n\n\nmin\n0.200000\n0.000000\n0.000000\n0.000000\n43.000000\n43.000000\n326.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.400000\n2.000000\n2.000000\n2.000000\n61.000000\n56.000000\n950.000000\n4.710000\n4.720000\n2.910000\n\n\n50%\n0.700000\n3.000000\n3.000000\n3.000000\n61.800000\n57.000000\n2401.000000\n5.700000\n5.710000\n3.530000\n\n\n75%\n1.040000\n4.000000\n5.000000\n4.000000\n62.500000\n59.000000\n5324.250000\n6.540000\n6.540000\n4.040000\n\n\nmax\n5.010000\n4.000000\n6.000000\n7.000000\n79.000000\n95.000000\n18823.000000\n10.740000\n58.900000\n31.800000\n\n\n\n\n\n\n\n\n\nEliminate disturbing values\n\ndiamond_df = diamond_df.drop(diamond_df[diamond_df[\"x\"]==0].index)\ndiamond_df = diamond_df.drop(diamond_df[diamond_df[\"y\"]==0].index)\ndiamond_df = diamond_df.drop(diamond_df[diamond_df[\"z\"]==0].index)\n\n\n\nEliminate values that &gt; 99% of the rest\n\ndiamond_df = diamond_df[diamond_df['depth'] &lt; diamond_df['depth'].quantile(0.99)]\ndiamond_df = diamond_df[diamond_df['table'] &lt; diamond_df['table'].quantile(0.99)]\ndiamond_df = diamond_df[diamond_df['x'] &lt; diamond_df['x'].quantile(0.99)]\ndiamond_df = diamond_df[diamond_df['y'] &lt; diamond_df['y'].quantile(0.99)]\ndiamond_df = diamond_df[diamond_df['z'] &lt; diamond_df['z'].quantile(0.99)]\n\n\ndiamond_df.head(10)\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\n4\n5\n1\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\n3\n5\n2\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n3\n0.29\n3\n1\n3\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\n1\n0\n1\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n5\n0.24\n2\n0\n5\n62.8\n57.0\n336\n3.94\n3.96\n2.48\n\n\n6\n0.24\n2\n1\n6\n62.3\n57.0\n336\n3.95\n3.98\n2.47\n\n\n7\n0.26\n2\n2\n2\n61.9\n55.0\n337\n4.07\n4.11\n2.53\n\n\n8\n0.22\n0\n5\n3\n65.1\n61.0\n337\n3.87\n3.78\n2.49\n\n\n9\n0.23\n2\n2\n4\n59.4\n61.0\n338\n4.00\n4.05\n2.39\n\n\n10\n0.30\n1\n0\n2\n64.0\n55.0\n339\n4.25\n4.28\n2.73\n\n\n\n\n\n\n\n\ndiamond_df.describe()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\ncount\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n51130.000000\n\n\nmean\n0.748505\n2.965500\n3.463974\n3.117426\n61.717375\n57.349591\n3561.244631\n5.637999\n5.641865\n3.480219\n\n\nstd\n0.406608\n1.060319\n1.679777\n1.640241\n1.285391\n2.074312\n3475.346374\n1.035121\n1.028587\n0.637887\n\n\nmin\n0.200000\n0.000000\n0.000000\n0.000000\n43.000000\n43.000000\n326.000000\n3.730000\n3.680000\n1.070000\n\n\n25%\n0.390000\n2.000000\n2.000000\n2.000000\n61.100000\n56.000000\n921.000000\n4.680000\n4.690000\n2.890000\n\n\n50%\n0.700000\n3.000000\n3.000000\n3.000000\n61.800000\n57.000000\n2273.000000\n5.640000\n5.650000\n3.480000\n\n\n75%\n1.020000\n4.000000\n5.000000\n4.000000\n62.500000\n59.000000\n4997.000000\n6.480000\n6.480000\n4.010000\n\n\nmax\n2.070000\n4.000000\n6.000000\n7.000000\n65.500000\n63.500000\n18806.000000\n8.300000\n8.150000\n5.000000\n\n\n\n\n\n\n\n\nX = diamond_df.drop(['price'], axis=1)\ny = diamond_df['price']\n\n\nX = X.to_numpy()\ny = y.to_numpy()\n\n\nX.shape, y.shape\n\n((51130, 9), (51130,))\n\n\n\n\nData for Training and Testing\nSplit dataset into 80% training and 20% testing.\n\nX_train = X[:int(X.shape[0]*0.8)]\ny_train = y[:int(X.shape[0]*0.8)]\nX_test = X[int(X.shape[0]*0.8):]\ny_test = y[int(X.shape[0]*0.8):]\n\nNormailize the data by subtracting the mean and dividing by the standard deviation.\n\nxmean = np.mean(X_train, axis=0)\nxstd = np.std(X_train, axis=0)\nX_train = (X_train - xmean) / xstd\nX_test = (X_test - xmean) / xstd\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((40904, 9), (10226, 9), (40904,), (10226,))\n\n\nTranpose the data so that each row is a feature and each column is an example.\n\nX_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\nX_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((40904, 10), (10226, 10), (40904,), (10226,))"
  },
  {
    "objectID": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#modeling",
    "href": "notebooks/regression/diamond_price_prediction/diamond_price_prediction.html#modeling",
    "title": "",
    "section": "Modeling",
    "text": "Modeling\n\nLinear Regression Model\n\nN = X_train.shape[0]\nn_epochs = 1000\nm = 1000\nlearning_rate = 0.001\n\n# No explicit bias term; the first column of X_* is ones to learn the intercept via W\ntheta = np.random.randn(10, 1)\nlosses = []\n\n\n\nTraining\n\nfor epoch in range(n_epochs):\n    for i in range(0, N, m):\n        # Take a batch of data\n        X_batch = X_train[i:i+m, :]\n        y_batch = y_train[i:i+m].reshape(-1, 1)\n        \n        # Predict y_hat\n        y_hat = X_batch @ theta\n        \n        # Compute loss\n        loss = np.mean((y_hat - y_batch) ** 2)\n        losses.append(loss)\n        \n        # Compute gradient\n        gradient = 2 * X_batch.T @ (y_hat - y_batch)\n        \n        # Update weights\n        theta -= learning_rate * (gradient / m)\n        \n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch {epoch + 1}/{n_epochs} - Loss: {losses[-1]}\")\n\nEpoch 50/1000 - Loss: 854708.0516617359\nEpoch 100/1000 - Loss: 930914.8572299478\nEpoch 150/1000 - Loss: 898110.7162777601\nEpoch 200/1000 - Loss: 865845.7363739273\nEpoch 250/1000 - Loss: 837663.5253486462\nEpoch 300/1000 - Loss: 813167.4802514835\nEpoch 350/1000 - Loss: 791865.9880215391\nEpoch 400/1000 - Loss: 773328.4771558258\nEpoch 450/1000 - Loss: 757183.2714683163\nEpoch 500/1000 - Loss: 743109.6553640236\nEpoch 550/1000 - Loss: 730830.8144026892\nEpoch 600/1000 - Loss: 720107.7571102347\nEpoch 650/1000 - Loss: 710734.0840505699\nEpoch 700/1000 - Loss: 702531.4850563508\nEpoch 750/1000 - Loss: 695345.8628449123\nEpoch 800/1000 - Loss: 689043.9955747422\nEpoch 850/1000 - Loss: 683510.6631254262\nEpoch 900/1000 - Loss: 678646.17239842\nEpoch 950/1000 - Loss: 674364.2259820343\nEpoch 1000/1000 - Loss: 670590.0863051731\n\n\n\n# Validate the model, compute MSE on the test set\ny_hat_test = X_test @ theta\ntest_loss_mse = np.mean((y_test - y_hat_test) ** 2)\nprint(f\"Test MSE: {test_loss_mse}\")\n\nTest MSE: 1537603.700873669\n\n\n\n# MAE\ntest_loss_mae = np.mean(np.abs(y_test - y_hat_test))\nprint(f\"Test MAE: {test_loss_mae}\")\n\nTest MAE: 955.1539844066257\n\n\n\n# Save the model parameters\nnp.savez(\n    'weight.npz',\n    x_mean=xmean,\n    x_std=xstd,\n    theta=theta\n)\n\n\n# Save the training and testing data\nnp.savez('data.npz', X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
  },
  {
    "objectID": "notebooks/regression/regression_with_single_perceptron/regression_with_single_perceptron.html",
    "href": "notebooks/regression/regression_with_single_perceptron/regression_with_single_perceptron.html",
    "title": "",
    "section": "",
    "text": "Table of contents\n- 1 - Simple Linear Regression\n- - Simple Linear Regression Model\n- - Neural Network Model with a Single Perceptron and One Input Node\n- Dataset\n- 2 - Multiple Linear Regression\n- - Neural Network Model with a Single Perceptron and Two Input Nodes\n\n\n ## 1 - Simple Linear Regression\n ### - Simple Linear Regression Model\nYou can describe a simple linear regression model as\n\\[\\hat{y} = wx + b,\\tag{1}\\]\nwhere \\(\\hat{y}\\) is a prediction of dependent variable \\(y\\) based on independent variable \\(x\\) using a line equation with the slope \\(w\\) and intercept \\(b\\).\nGiven a set of training data points \\((x_1, y_1)\\), …, \\((x_m, y_m)\\), you will find the “best” fitting line - such parameters \\(w\\) and \\(b\\) that the differences between original values \\(y_i\\) and predicted values \\(\\hat{y}_i = wx_i + b\\) are minimum.\n ### - Neural Network Model with a Single Perceptron and One Input Node\nThe simplest neural network model that describes the above problem can be realized by using one perceptron. The input and output layers will have one node each (\\(x\\) for input and \\(\\hat{y} = z\\) for output):\n\nWeight (\\(w\\)) and bias (\\(b\\)) are the parameters that will get updated when you train the model. They are initialized to some random values or set to 0 and updated as the training progresses.\nFor each training example \\(x^{(i)}\\), the prediction \\(\\hat{y}^{(i)}\\) can be calculated as:\n\\[\\begin{align}\nz^{(i)} &=  w x^{(i)} + b,\\\\\n\\hat{y}^{(i)} &= z^{(i)},\n\\tag{2}\\end{align}\\]\nwhere \\(i = 1, \\dots, m\\).\nYou can organise all training examples as a vector \\(X\\) of size (\\(1 \\times m\\)) and perform scalar multiplication of \\(X\\) (\\(1 \\times m\\)) by a scalar \\(w\\), adding \\(b\\), which will be broadcasted to a vector of size (\\(1 \\times m\\)):\n\\[\\begin{align}\nZ &=  w X + b,\\\\\n\\hat{Y} &= Z,\n\\tag{3}\\end{align}\\]\nThis set of calculations is called forward propagation.\nFor each training example you can measure the difference between original values \\(y^{(i)}\\) and predicted values \\(\\hat{y}^{(i)}\\) with the loss function \\(L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2\\). Division by \\(2\\) is taken just for scaling purposes, you will see the reason below, calculating partial derivatives. To compare the resulting vector of the predictions \\(\\hat{Y}\\) (\\(1 \\times m\\)) with the vector \\(Y\\) of original values \\(y^{(i)}\\), you can take an average of the loss function values for each of the training examples:\n\\[\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.\\tag{4}\\]\nThis function is called the sum of squares cost function. The aim is to optimize the cost function during the training, which will minimize the differences between original values \\(y^{(i)}\\) and predicted values \\(\\hat{y}^{(i)}\\).\nWhen your weights were just initialized with some random values, and no training was done yet, you can’t expect good results. You need to calculate the adjustments for the weight and bias, minimizing the cost function. This process is called backward propagation.\nAccording to the gradient descent algorithm, you can calculate partial derivatives as:\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L} }{ \\partial w } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)},\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial b } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right).\n\\tag{5}\\end{align}\\]\nYou can see how the additional division by \\(2\\) in the equation \\((4)\\) helped to simplify the results of the partial derivatives. Then update the parameters iteratively using the expressions\n\\[\\begin{align}\nw &= w - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial w },\\\\\nb &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b },\n\\tag{6}\\end{align}\\]\nwhere \\(\\alpha\\) is the learning rate. Then repeat the process until the cost function stops decreasing.\nThe general methodology to build a neural network is to: 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model’s parameters 3. Loop: - Implement forward propagation (calculate the perceptron output), - Implement backward propagation (to get the required corrections for the parameters), - Update parameters. 4. Make predictions.\nYou often build helper functions to compute steps 1-3 and then merge them into one function nn_model(). Once you’ve built nn_model() and learnt the right parameters, you can make predictions on new data.\n\n# Import all the required packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n# A library for data manipulation and analysis\nimport pandas as pd\n\n# Output of plotting commands is displayed within the Jupyter notebook\n%matplotlib inline\n\n# Set a seed so that the results are consistent accross sessions.\nnp.random.seed(3)\n\n\nDataset\nLoad the Kaggle dataset, saved in ../data/tvmarketing.csv. It has two fields: TV marketing expenses (TV) and sales amount (Sales).\n\npath = '../../data/regression_with_single_perceptron/tvmarketing.csv'\nadv = pd.read_csv(path)\n\nPrint some part of the dataset\n\nadv.head()\n\n\n\n\n\n\n\n\nTV\nSales\n\n\n\n\n0\n230.1\n22.1\n\n\n1\n44.5\n10.4\n\n\n2\n17.2\n9.3\n\n\n3\n151.5\n18.5\n\n\n4\n180.8\n12.9\n\n\n\n\n\n\n\nPlot the data points to see how they are distributed. You can use plt.plot() function from matplotlib.pyplot module.\n\nadv.plot(x='TV', y='Sales', kind='scatter', c='black')\n\n\n\n\n\n\n\n\nNormalize the data by subtracting the mean and dividing by the standard deviation. This will help the model to converge faster during training, and to avoid different units of measurement in the input data.\nNormalized data have mean 0 and standard deviation 1. The following cell performs column-wise normalization of the dataset:\n\nadv_norm = (adv - np.mean(adv, axis=0)) / np.std(adv, axis=0) # As omitting axis for np.std() is depracated, to avoid error, we must pass axis\nadv_norm.head()\n\n\n\n\n\n\n\n\nTV\nSales\n\n\n\n\n0\n0.969852\n1.552053\n\n\n1\n-1.197376\n-0.696046\n\n\n2\n-1.516155\n-0.907406\n\n\n3\n0.052050\n0.860330\n\n\n4\n0.394182\n-0.215683\n\n\n\n\n\n\n\n\nX_norm = adv_norm['TV'] # type pd.Series\nY_norm = adv_norm['Sales'] # type pd.Series\n\n# Convert to np.array first to be able to reshape\nX_norm = np.array(X_norm).reshape(1, -1)\nY_norm = np.array(Y_norm).reshape(1, -1)\n\nX_norm.shape, Y_norm.shape\n\n((1, 200), (1, 200))\n\n\nPlot the data points to see how they are distributed.\n\nadv_norm.plot(x='TV', y='Sales', kind='scatter', c='black')\n\n\n\n\n\n\n\n\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_y -- the size of the output layer\n    \"\"\"\n    \n    n_x, n_y = X.shape[0], Y.shape[0]\n    \n    return (n_x, n_y)\n\nn_x, n_y = layer_sizes(X_norm, Y_norm)\nprint(n_x, n_y)\n\n1 1\n\n\n\ndef initialize_parameters(n_x, n_y):\n    \"\"\"\n    Returns:\n    params -- python dictionary containing your parameters:\n                    W -- weight matrix of shape (n_y, n_x)\n                    b -- bias value set as a vector of shape (n_y, 1)\n    \"\"\"\n    \n    W = np.random.randn(n_y, n_x) * 0.01 # scales down with std of 0.01 to avoid large computations later on\n    b = np.zeros((n_y, 1))\n    \n    params = {\"W\": W,\n              \"b\": b}\n    \n    return params\n\nparameters = initialize_parameters(n_x, n_y)\nprint(parameters[\"W\"], parameters[\"b\"])\n\n[[0.01788628]] [[0.]]\n\n\nImplement forward_propagation() following the equation \\((3)\\) in the section 1.2: \\[\\begin{align}\nZ &=  w X + b\\\\\n\\hat{Y} &= Z,\n\\end{align}\\]\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    Y_hat -- The output of size (n_y, m)\n    \"\"\"\n    \n    W = parameters[\"W\"]\n    b = parameters[\"b\"]\n    Y_hat = W @ X + b\n    \n    return Y_hat\n\nY_hat = forward_propagation(X_norm, parameters)\nprint(\"Some element of predicted Y_hat values: \", Y_hat[0, :5])\n\nSome element of predicted Y_hat values:  [ 0.01734705 -0.02141661 -0.02711838  0.00093098  0.00705046]\n\n\nYour weights were just initialized with some random values, so the model has not been trained yet.\nDefine a cost function \\((4)\\) which will be used to train the model:\n\\[\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2\\]\n\ndef compute_cost(Y_hat, Y):\n    \"\"\"\n    Computes the cost function as a sum of squares\n    \n    Arguments:\n    Y_hat -- The output of the neural network of shape (n_y, number of examples)\n    Y -- \"true\" labels vector of shape (n_y, number of examples)\n    \n    Returns:\n    cost -- sum of squares scaled by 1/(2*number of examples)\n    \n    \"\"\"\n    \n    # Number of examples\n    m = Y_hat.shape[1]\n    \n    # Compute cost\n    cost = np.sum((Y_hat - Y)**2) / (2*m)\n    \n    return cost\n\nprint(f\"cost = {str(compute_cost(Y_hat, Y_norm))}\")\n\ncost = 0.48616887080159704\n\n\nCalculate partial derivatives as shown in \\((5)\\):\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L} }{ \\partial w } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)},\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial b } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right).\n\\end{align}\\]\n\ndef backward_propagation(Y_hat, X, Y):\n    \"\"\"\n    Implements the backward propagation, calculating gradients\n    \n    Arguments:\n    Y_hat -- the output of the neural network of shape (n_y, number of examples)\n    X -- input data of shape (n_x, number of examples)\n    Y -- \"true\" labels vector of shape (n_y, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing gradients with respect to different parameters\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    dZ = Y_hat - Y\n    dW = 1/m * (dZ @ X.T)\n    db = 1/m * np.sum(dZ, axis=1, keepdims=True) # Sum over rows, and do not reduce dimensions. e.g. (n_y,) -&gt; (n_y, 1)\n    \n    grads = {\"dW\": dW,\n             \"db\": db}\n    \n    return grads\n\ngrads = backward_propagation(Y_hat, X_norm, Y_norm)\nprint(\"dW = \" + str(grads[\"dW\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n\ndW = [[-0.76433814]]\ndb = [[1.687539e-16]]\n\n\nUpdate parameters as shown in \\((6)\\):\n\\[\\begin{align}\nw &= w - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial w },\\\\\nb &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b }.\n\\end{align}\\]\n\ndef update_parameters(parameters, grads, learning_rate=1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule\n    \n    Arguments:\n    parameters -- python dictionary containing parameters \n    grads -- python dictionary containing gradients \n    learning_rate -- learning rate parameter for gradient descent\n    \n    Returns:\n    parameters -- python dictionary containing updated parameters \n    \"\"\"\n    \n    W = parameters[\"W\"] - learning_rate * grads[\"dW\"]\n    b = parameters[\"b\"] - learning_rate * grads[\"db\"]\n    \n    parameters = {\"W\": W,\n                  \"b\": b}\n    \n    return parameters\n\nupdated_params = update_parameters(parameters, grads)\nprint(\"W updated = \" + str(updated_params[\"W\"]))\nprint(\"b updated = \" + str(updated_params[\"b\"]))\n\nW updated = [[0.93509205]]\nb updated = [[-2.0250468e-16]]\n\n\nPut everything together in the function nn_model().\n\ndef nn_model(X, Y, num_iterations=10, learning_rate=1.2, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (n_x, number of examples)\n    Y -- labels of shape (n_y, number of examples)\n    num_iterations -- number of iterations in the loop\n    learning_rate -- learning rate parameter for gradient descent\n    print_cost -- if True, print the cost every iteration\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to make predictions.\n    \"\"\"\n    \n    n_x, n_y = layer_sizes(X, Y)\n    \n    parameters = initialize_parameters(n_x, n_y)\n    \n    for i in range(num_iterations):\n        \n        # Forward propagation. Inputs: \"X, parameters\". Output: \"Y_hat\".\n        Y_hat = forward_propagation(X, parameters)\n        \n        # Cost function. Inputs: \"Y_hat, Y\". Output: \"cost\".\n        cost = compute_cost(Y_hat, Y)\n        \n        # Backpropagation. Inputs: \"Y_hat, X, Y\". Output: \"grads\".\n        grads = backward_propagation(Y_hat, X, Y)\n        \n        # Gradient descent parameter update. Inputs: \"parameters, grads, learning_rate\". Output: \"parameters\".\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        if print_cost:\n            print(f\"Cost after iteration {i}: {cost}\")\n    \n    return parameters    \n\n\nparameters_simple = nn_model(X_norm, Y_norm, num_iterations=30, learning_rate=1.2, print_cost=True)\nprint(\"W = \" + str(parameters_simple[\"W\"]))\nprint(\"b = \" + str(parameters_simple[\"b\"]))\n\nW_simple = parameters[\"W\"]\nb_simple = parameters[\"b\"]\n\nCost after iteration 0: 0.49659504037484803\nCost after iteration 1: 0.20616377720695983\nCost after iteration 2: 0.1945465266802443\nCost after iteration 3: 0.1940818366591757\nCost after iteration 4: 0.19406324905833294\nCost after iteration 5: 0.19406250555429921\nCost after iteration 6: 0.19406247581413788\nCost after iteration 7: 0.19406247462453144\nCost after iteration 8: 0.19406247457694714\nCost after iteration 9: 0.1940624745750438\nCost after iteration 10: 0.19406247457496764\nCost after iteration 11: 0.19406247457496462\nCost after iteration 12: 0.19406247457496448\nCost after iteration 13: 0.19406247457496448\nCost after iteration 14: 0.19406247457496448\nCost after iteration 15: 0.19406247457496448\nCost after iteration 16: 0.19406247457496448\nCost after iteration 17: 0.19406247457496448\nCost after iteration 18: 0.19406247457496448\nCost after iteration 19: 0.19406247457496448\nCost after iteration 20: 0.19406247457496448\nCost after iteration 21: 0.19406247457496448\nCost after iteration 22: 0.19406247457496448\nCost after iteration 23: 0.19406247457496448\nCost after iteration 24: 0.19406247457496448\nCost after iteration 25: 0.19406247457496448\nCost after iteration 26: 0.19406247457496448\nCost after iteration 27: 0.19406247457496448\nCost after iteration 28: 0.19406247457496448\nCost after iteration 29: 0.19406247457496448\nW = [[0.78222442]]\nb = [[-3.19744231e-16]]\n\n\n\ndef predict(X, Y, parameters, X_pred):\n    W = parameters[\"W\"]\n    b = parameters[\"b\"]\n    \n    # Use the same mean and standard deviation of the original training array X.\n    \"\"\"\n        Handling of X for normalization:\n\n        - If X is a Pandas Series:\n            np.mean(X) and np.std(X) return scalar values (shape ()), \n            because a Series is essentially a 1D array of values. \n            This case typically corresponds to having only one feature column.\n            In this scenario, we store mean and std as scalars and normalize X_pred \n            using these scalars, then reshape it to (1, len(X_pred)) so that it \n            matches the expected shape for matrix multiplication with W.\n\n        - If X is a Pandas DataFrame:\n            np.mean(X) and np.std(X) return a Pandas Series containing \n            column-wise means and standard deviations. Converting them to NumPy \n            arrays produces a 1D vector of shape (n_features,). \n            We reshape them to (n_features, 1) so they can be broadcasted \n            correctly during normalization of X_pred. \n            This case typically corresponds to having multiple feature columns.\n    \"\"\"\n    \n    if isinstance(X, pd.Series):\n        X_mean = np.mean(X)\n        X_std = np.std(X)\n        X_pred_norm = ((X_pred - X_mean) / X_std).reshape((1, len(X_pred)))\n    else:\n        X_mean = np.array(np.mean(X, axis=0)).reshape((len(X.axes[1]), 1))\n        X_std = np.array(np.std(X, axis=0)).reshape((len(X.axes[1]), 1))\n        X_pred_norm = (X_pred - X_mean) / X_std\n        \n    # Make predictions\n    Y_pred_norm = W @ X_pred_norm + b\n    \n    # Convert back using same mean and std of original training Y\n    Y_pred = Y_pred_norm * np.std(Y) + np.mean(Y)\n    \n    return Y_pred[0]\n\nX_pred = np.array([230.1, 44.2, 8.6])\nY_pred = predict(adv[\"TV\"], adv[\"Sales\"], parameters_simple, X_pred)\nprint(f\"TV marketing expenses:\\n{X_pred}\")\nprint(f\"Predictions of sales:\\n{Y_pred}\")\n\nTV marketing expenses:\n[230.1  44.2   8.6]\nPredictions of sales:\n[17.97077451  9.13371306  7.44140866]\n\n\n\nfig, ax = plt.subplots()\nax.scatter(adv[\"TV\"], adv[\"Sales\"], c=\"black\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(\"$y$\")\n\nX_line = np.arange(np.min(adv[\"TV\"]), np.max(adv[\"TV\"])*1.1, 0.1)\nY_line = predict(adv[\"TV\"], adv[\"Sales\"], parameters_simple, X_line)\nax.plot(X_line, Y_line, \"r\")\nax.plot(X_pred, Y_pred, \"bo\") # blue dots\nplt.plot()\nplt.show()\n\n\n\n\n\n\n\n\n ## 2 - Multiple Linear Regression  ### - Neural Network Model with a Single Perceptron and Two Input Nodes\nLet’s build a linear regression model for a Kaggle dataset House Prices, saved in a file ..data/house_prices_train.csv. You will use two fields - ground living area (GrLivArea, square feet) and rates of the overall quality of material and finish (OverallQual, 1-10) to predict sales price (SalePrice, dollars).\n\npath = '../../data/regression_with_single_perceptron/house_prices_train.csv'\ndf = pd.read_csv(path)\n\n\n\nX_multi = df[['GrLivArea', 'OverallQual']]\nY_multi = df['SalePrice']\nX_multi.shape, Y_multi.shape\n\n((1460, 2), (1460,))\n\n\n\ndisplay(X_multi)\ndisplay(Y_multi)\n\n\n\n\n\n\n\n\nGrLivArea\nOverallQual\n\n\n\n\n0\n1710\n7\n\n\n1\n1262\n6\n\n\n2\n1786\n7\n\n\n3\n1717\n7\n\n\n4\n2198\n8\n\n\n...\n...\n...\n\n\n1455\n1647\n6\n\n\n1456\n2073\n6\n\n\n1457\n2340\n7\n\n\n1458\n1078\n5\n\n\n1459\n1256\n5\n\n\n\n\n1460 rows × 2 columns\n\n\n\n0       208500\n1       181500\n2       223500\n3       140000\n4       250000\n         ...  \n1455    175000\n1456    210000\n1457    266500\n1458    142125\n1459    147500\nName: SalePrice, Length: 1460, dtype: int64\n\n\n\n\nX_multi_norm = (X_multi - np.mean(X_multi, axis=0)) / np.std(X_multi, axis=0)\nY_multi_norm = (Y_multi - np.mean(Y_multi)) / np.std(Y_multi, axis=0)\nX_multi_norm.shape, Y_norm.shape\n\n((1460, 2), (1, 200))\n\n\n\nX_multi_norm = np.array(X_multi_norm).T\nY_multi_norm = np.array(Y_multi_norm).reshape((1, len(Y_multi_norm)))\nX_norm.shape, Y_multi_norm.shape\n\n((1, 200), (1, 1460))\n\n\n\nprint(X_multi_norm[:, :5], Y_multi_norm[0, :5])\n\n[[ 0.37033344 -0.48251191  0.51501256  0.38365915  1.2993257 ]\n [ 0.65147924 -0.07183611  0.65147924  0.65147924  1.3747946 ]] [ 0.34727322  0.00728832  0.53615372 -0.51528106  0.8698426 ]\n\n\n\nparameters_multi = nn_model(X_multi_norm, Y_multi_norm, num_iterations=100, print_cost=True)\n\nW = parameters_multi[\"W\"]\nb = parameters_multi[\"b\"]\nprint(f\"W: {parameters_multi['W']} and b: {parameters_multi['b']}\")\n\nCost after iteration 0: 0.5142195119760039\nCost after iteration 1: 0.44862664805802294\nCost after iteration 2: 0.3962236237408893\nCost after iteration 3: 0.35322650244442194\nCost after iteration 4: 0.3176388968906906\nCost after iteration 5: 0.28810227675703015\nCost after iteration 6: 0.2635663230551014\nCost after iteration 7: 0.2431787775092655\nCost after iteration 8: 0.22623677631465744\nCost after iteration 9: 0.21215762854913936\nCost after iteration 10: 0.20045746930524147\nCost after iteration 11: 0.19073428857122826\nCost after iteration 12: 0.1826540289332285\nCost after iteration 13: 0.1759390850066516\nCost after iteration 14: 0.170358759895168\nCost after iteration 15: 0.1657213378223942\nCost after iteration 16: 0.16186749718900195\nCost after iteration 17: 0.15866483686740238\nCost after iteration 18: 0.1560033274486568\nCost after iteration 19: 0.1537915311054225\nCost after iteration 20: 0.1519534601860057\nCost after iteration 21: 0.15042596661037508\nCost after iteration 22: 0.14915657237810437\nCost after iteration 23: 0.14810166665369826\nCost after iteration 24: 0.1472250074889603\nCost after iteration 25: 0.14649647670811547\nCost after iteration 26: 0.14589104517897944\nCost after iteration 27: 0.14538791292141448\nCost after iteration 28: 0.14496979451095832\nCost after iteration 29: 0.1446223252272187\nCost after iteration 30: 0.14433356754488944\nCost after iteration 31: 0.1440936010125767\nCost after iteration 32: 0.14389418142946578\nCost after iteration 33: 0.1437284576106279\nCost after iteration 34: 0.1435907360102627\nCost after iteration 35: 0.14347628511635882\nCost after iteration 36: 0.1433811728966296\nCost after iteration 37: 0.14330213171107611\nCost after iteration 38: 0.1432364460501643\nCost after iteration 39: 0.14318185924179086\nCost after iteration 40: 0.1431364959218967\nCost after iteration 41: 0.14309879760515798\nCost after iteration 42: 0.14306746914224452\nCost after iteration 43: 0.14304143422415205\nCost after iteration 44: 0.1430197984049305\nCost after iteration 45: 0.1430018183724325\nCost after iteration 46: 0.1429868764113577\nCost after iteration 47: 0.14297445918125612\nCost after iteration 48: 0.14296414008039493\nCost after iteration 49: 0.142955564589588\nCost after iteration 50: 0.1429484380924669\nCost after iteration 51: 0.14294251575375058\nCost after iteration 52: 0.14293759410777576\nCost after iteration 53: 0.14293350406830627\nCost after iteration 54: 0.1429301051194693\nCost after iteration 55: 0.1429272804882445\nCost after iteration 56: 0.1429249331326534\nCost after iteration 57: 0.14292298240782159\nCost after iteration 58: 0.14292136129537322\nCost after iteration 59: 0.14292001410097277\nCost after iteration 60: 0.14291889454091114\nCost after iteration 61: 0.14291796415199975\nCost after iteration 62: 0.14291719097014374\nCost after iteration 63: 0.14291654843219517\nCost after iteration 64: 0.14291601446335958\nCost after iteration 65: 0.14291557071880256\nCost after iteration 66: 0.14291520195340132\nCost after iteration 67: 0.1429148954979891\nCost after iteration 68: 0.1429146408240979\nCost after iteration 69: 0.1429144291822463\nCost after iteration 70: 0.1429142533013457\nCost after iteration 71: 0.14291410713889743\nCost after iteration 72: 0.14291398567339883\nCost after iteration 73: 0.14291388473182634\nCost after iteration 74: 0.14291380084626906\nCost after iteration 75: 0.14291373113478598\nCost after iteration 76: 0.1429136732023953\nCost after iteration 77: 0.14291362505879346\nCost after iteration 78: 0.14291358504997667\nCost after iteration 79: 0.14291355180141707\nCost after iteration 80: 0.1429135241708394\nCost after iteration 81: 0.14291350120897797\nCost after iteration 82: 0.1429134821269635\nCost after iteration 83: 0.1429134662692215\nCost after iteration 84: 0.14291345309094916\nCost after iteration 85: 0.14291344213939877\nCost after iteration 86: 0.14291343303832293\nCost after iteration 87: 0.14291342547504843\nCost after iteration 88: 0.14291341918973427\nCost after iteration 89: 0.1429134139664445\nCost after iteration 90: 0.14291340962572974\nCost after iteration 91: 0.142913406018462\nCost after iteration 92: 0.14291340302071137\nCost after iteration 93: 0.14291340052948823\nCost after iteration 94: 0.142913398459205\nCost after iteration 95: 0.14291339673873585\nCost after iteration 96: 0.14291339530897298\nCost after iteration 97: 0.14291339412079596\nCost after iteration 98: 0.14291339313338425\nCost after iteration 99: 0.1429133923128147\nW: [[0.3694604  0.57181575]] and b: [[1.21181604e-16]]\n\n\n\nX_multi_pred = np.array([[1710, 7], [1200, 6], [2200, 8]]).T\nY_multi_pred = predict(X_multi, Y_multi, parameters_multi, X_multi_pred)\n\nprint(f\"Ground living area, square feet:\\n{X_multi_pred[0]}\")\nprint(f\"Rates of the overall quality of material and finish, 1-10:\\n{X_multi_pred[1]}\")\nprint(f\"Predictions of sales price, $:\\n{np.round(Y_multi_pred)}\")\n\nGround living area, square feet:\n[1710 1200 2200]\nRates of the overall quality of material and finish, 1-10:\n[7 6 8]\nPredictions of sales price, $:\n[221371. 160039. 281587.]\n\n\n\n# Ensure X_multi and Y_multi exist; if not, load\ntry:\n    X_multi\n    Y_multi\nexcept NameError:\n    df = pd.read_csv('../data/house_prices_train.csv')\n    X_multi = df[['GrLivArea', 'OverallQual']]\n    Y_multi = df['SalePrice']\n\n# Ensure trained parameters are available (no retraining here)\nif 'parameters_multi' not in globals():\n    raise RuntimeError(\"Trained parameters 'parameters_multi' not found. Please run the training cell above first.\")\n\n# Build prediction grids while fixing the other feature at its mean (original scale)\nx1 = np.linspace(X_multi['GrLivArea'].min(), X_multi['GrLivArea'].max() * 1.1, 200)\nx2_fixed = float(X_multi['OverallQual'].mean())\nX_pred1 = np.vstack([x1, np.full_like(x1, x2_fixed)])  # shape (2, 200)\ny1 = predict(X_multi, Y_multi, parameters_multi, X_pred1)\n\nx2 = np.linspace(X_multi['OverallQual'].min(), X_multi['OverallQual'].max() * 1.1, 200)\nx1_fixed = float(X_multi['GrLivArea'].mean())\nX_pred2 = np.vstack([np.full_like(x2, x1_fixed), x2])  # shape (2, 200)\ny2 = predict(X_multi, Y_multi, parameters_multi, X_pred2)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Subplot 1: SalePrice vs GrLivArea\naxes[0].scatter(X_multi['GrLivArea'], Y_multi, s=12, alpha=0.6, color='black', label='Data')\naxes[0].plot(x1, y1, color='red', linewidth=2, label='Predicted')\naxes[0].set_xlabel('GrLivArea (sq ft)')\naxes[0].set_ylabel('SalePrice ($)')\naxes[0].set_title('SalePrice vs GrLivArea')\naxes[0].legend(loc='best')\n\n# Subplot 2: SalePrice vs OverallQual\naxes[1].scatter(X_multi['OverallQual'], Y_multi, s=12, alpha=0.6, color='black', label='Data')\naxes[1].plot(x2, y2, color='red', linewidth=2, label='Predicted')\naxes[1].set_xlabel('OverallQual (1-10)')\naxes[1].set_title('SalePrice vs OverallQual')\naxes[1].legend(loc='best')\n\nfig.suptitle('House Prices: Feature-wise Scatter with Predicted Lines', y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/classification/classification_with_single_perceptron/classification_with_single_perceptron.html",
    "href": "notebooks/classification/classification_with_single_perceptron/classification_with_single_perceptron.html",
    "title": "Classification with Perceptron",
    "section": "",
    "text": "Table of contents\n- Classification with Perceptron\n- Packages\n- 1 - Single Perceptron Neural Network with Activation Function\n- - Neural Network Structure\n- - Dataset\n- - Define Activation Function\n- 2 - Implementation of the Neural Network Model\n- 3 - Performance on a Larger Dataset"
  },
  {
    "objectID": "notebooks/classification/classification_with_single_perceptron/classification_with_single_perceptron.html#packages",
    "href": "notebooks/classification/classification_with_single_perceptron/classification_with_single_perceptron.html#packages",
    "title": "Classification with Perceptron",
    "section": "Packages",
    "text": "Packages\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n# A function to create a dataset.\nfrom sklearn.datasets import make_blobs \n\n# Output of plotting commands is displayed inline within the Jupyter notebook.\n%matplotlib inline \n\n# Set a seed so that the results are consistent.\nnp.random.seed(3)\n\n ## 1 - Single Perceptron Neural Network with Activation Function\nYou already have constructed and trained a neural network model with one perceptron. Here a similar model can be used, but with an activation function. Then a single perceptron basically works as a threshold function.\n ### - Neural Network Structure\nThe neural network components are shown in the following scheme:\n\nSimilarly to the previous lab, the input layer contains two nodes \\(x_1\\) and \\(x_2\\). Weight vector \\(W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}\\) and bias (\\(b\\)) are the parameters to be updated during the model training. First step in the forward propagation is the same as in the previous lab. For every training example \\(x^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)}\\end{bmatrix}\\):\n\\[z^{(i)} = w_1x_1^{(i)} + w_2x_2^{(i)} + b = Wx^{(i)} + b.\\tag{1}\\]\nBut now you cannot take a real number \\(z^{(i)}\\) into the output as you need to perform classification. It could be done with a discrete approach: compare the result with zero, and classify as \\(0\\) (blue) if it is below zero and \\(1\\) (red) if it is above zero. Then define cost function as a percentage of incorrectly identified classes and perform backward propagation.\nThis extra step in the forward propagation is actually an application of an activation function. It would be possible to implement the discrete approach described above (with unit step function) for this problem, but it turns out that there is a continuous approach that works better and is commonly used in more complicated neural networks. So you will implement it here: single perceptron with sigmoid activation function.\nSigmoid activation function is defined as\n\\[a = \\sigma\\left(z\\right) = \\frac{1}{1+e^{-z}}.\\tag{2}\\]\nThen a threshold value of \\(0.5\\) can be used for predictions: \\(1\\) (red) if \\(a &gt; 0.5\\) and \\(0\\) (blue) otherwise. Putting it all together, mathematically the single perceptron neural network with sigmoid activation function can be expressed as:\n\\[\\begin{align}\nz^{(i)} &=  W x^{(i)} + b,\\\\\na^{(i)} &= \\sigma\\left(z^{(i)}\\right).\\\\\\tag{3}\n\\end{align}\\]\nIf you have \\(m\\) training examples organised in the columns of (\\(2 \\times m\\)) matrix \\(X\\), you can apply the activation function element-wise. So the model can be written as:\n\\[\\begin{align}\nZ &=  W X + b,\\\\\nA &= \\sigma\\left(Z\\right),\\\\\\tag{4}\n\\end{align}\\]\nwhere \\(b\\) is broadcasted to the vector of a size (\\(1 \\times m\\)).\nWhen dealing with classification problems, the most commonly used cost function is the log loss, which is described by the following equation:\n\\[\\mathcal{L}\\left(W, b\\right) = \\frac{1}{m}\\sum_{i=1}^{m} L\\left(W, b\\right) = \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small -y^{(i)}\\log\\left(a^{(i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{(i)}\\right)  \\large  \\right) \\small,\\tag{5}\\]\nwhere \\(y^{(i)} \\in \\{0,1\\}\\) are the original labels and \\(a^{(i)}\\) are the continuous output values of the forward propagation step (elements of array \\(A\\)).\nYou want to minimize the cost function during the training. To implement gradient descent, calculate partial derivatives using chain rule:\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L} }{ \\partial w_1 } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\frac{\\partial L }{ \\partial a^{(i)}}\n\\frac{\\partial a^{(i)} }{ \\partial z^{(i)}}\\frac{\\partial z^{(i)} }{ \\partial w_1},\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial w_2 } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\frac{\\partial L }{ \\partial a^{(i)}}\n\\frac{\\partial a^{(i)} }{ \\partial z^{(i)}}\\frac{\\partial z^{(i)} }{ \\partial w_2},\\tag{6}\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial b } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\frac{\\partial L }{ \\partial a^{(i)}}\n\\frac{\\partial a^{(i)} }{ \\partial z^{(i)}}\\frac{\\partial z^{(i)} }{ \\partial b}.\n\\end{align}\\]\nAs discussed in the videos, \\(\\frac{\\partial L }{ \\partial a^{(i)}}\n\\frac{\\partial a^{(i)} }{ \\partial z^{(i)}} = \\left(a^{(i)} - y^{(i)}\\right)\\), \\(\\frac{\\partial z^{(i)}}{ \\partial w_1} = x_1^{(i)}\\), \\(\\frac{\\partial z^{(i)}}{ \\partial w_2} = x_2^{(i)}\\) and \\(\\frac{\\partial z^{(i)}}{ \\partial b} = 1\\). Then \\((6)\\) can be rewritten as:\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L} }{ \\partial w_1 } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(a^{(i)} - y^{(i)}\\right)x_1^{(i)},\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial w_2 } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(a^{(i)} - y^{(i)}\\right)x_2^{(i)},\\tag{7}\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial b } &=\n\\frac{1}{m}\\sum_{i=1}^{m} \\left(a^{(i)} - y^{(i)}\\right).\n\\end{align}\\]\nNote that the obtained expressions \\((7)\\) are exactly the same as in the section \\(3.2\\) of the previous lab, when multiple linear regression model was discussed. Thus, they can be rewritten in a matrix form:\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L} }{ \\partial W } &=\n\\begin{bmatrix} \\frac{\\partial \\mathcal{L} }{ \\partial w_1 } &\n\\frac{\\partial \\mathcal{L} }{ \\partial w_2 }\\end{bmatrix} = \\frac{1}{m}\\left(A - Y\\right)X^T,\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \\frac{1}{m}\\left(A - Y\\right)\\mathbf{1}.\n\\tag{8}\n\\end{align}\\]\nwhere \\(\\left(A - Y\\right)\\) is an array of a shape (\\(1 \\times m\\)), \\(X^T\\) is an array of a shape (\\(m \\times 2\\)) and \\(\\mathbf{1}\\) is just a (\\(m \\times 1\\)) vector of ones.\nThen you can update the parameters:\n\\[\\begin{align}\nW &= W - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W },\\\\\nb &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b },\n\\tag{9}\\end{align}\\]\nwhere \\(\\alpha\\) is the learning rate. Repeat the process in a loop until the cost function stops decreasing.\nFinally, the predictions for some example \\(x\\) can be made taking the output \\(a\\) and calculating \\(\\hat{y}\\) as\n\\[\\hat{y} = \\begin{cases}\n1, & \\text{if } a \\ge 0.5 \\\\\n0, & \\text{otherwise}\n\\end{cases}\\tag{10}\\]\n ### - Dataset\nLet’s get the dataset you will work on. The following code will create \\(m=30\\) data points \\((x_1, x_2)\\), where \\(x_1, x_2 \\in \\{0,1\\}\\) and save them in the NumPy array X of a shape \\((2 \\times m)\\) (in the columns of the array). The labels (\\(0\\): blue, \\(1\\): red) will be calculated so that \\(y = 1\\) if \\(x_1 = 0\\) and \\(x_2 = 1\\), in the rest of the cases \\(y=0\\). The labels will be saved in the array Y of a shape \\((1 \\times m)\\).\n\nm = 30\n\nX = np.random.randint(0, 2, (2, m))\nY = np.logical_and(X[0] == 0, X[1] == 1).astype(int).reshape((1, m))\n\nprint('Training dataset X containing (x1, x2) coordinates in the columns:')\nprint(X)\nprint('Training dataset Y containing labels of two classes (0: blue, 1: red)')\nprint(Y)\n\nprint ('The shape of X is: ' + str(X.shape))\nprint ('The shape of Y is: ' + str(Y.shape))\nprint ('I have m = %d training examples!' % (X.shape[1]))\n\nTraining dataset X containing (x1, x2) coordinates in the columns:\n[[0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0]\n [0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0]]\nTraining dataset Y containing labels of two classes (0: blue, 1: red)\n[[0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0]]\nThe shape of X is: (2, 30)\nThe shape of Y is: (1, 30)\nI have m = 30 training examples!\n\n\n ### - Define Activation Function\nThe sigmoid function \\((2)\\) for a variable \\(z\\) can be defined with the following code:\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n    \nprint(\"sigmoid(-2) = \" + str(sigmoid(-2)))\nprint(\"sigmoid(0) = \" + str(sigmoid(0)))\nprint(\"sigmoid(3.5) = \" + str(sigmoid(3.5)))\n\nsigmoid(-2) = 0.11920292202211755\nsigmoid(0) = 0.5\nsigmoid(3.5) = 0.9706877692486436\n\n\nIt can be applied to a NumPy array element by element:\n\nprint(sigmoid(np.array([-2, 0, 3.5])))\n\n[0.11920292 0.5        0.97068777]\n\n\n ## 2 - Implementation of the Neural Network Model\nImplementation of the described neural network will be very similar to the regression_with_single_perceptron lab. The differences will be only in the functions forward_propagation and compute_cost!\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_y -- the size of the output layer\n    \"\"\"\n    n_x = X.shape[0]\n    n_y = Y.shape[0]\n    \n    return (n_x, n_y)\n\n(n_x, n_y) = layer_sizes(X, Y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))\n\nThe size of the input layer is: n_x = 2\nThe size of the output layer is: n_y = 1\n\n\n\ndef initialize_parameters(n_x, n_y):\n    \"\"\"\n    Returns:\n    params -- python dictionary containing your parameters:\n                    W -- weight matrix of shape (n_y, n_x)\n                    b -- bias value set as a vector of shape (n_y, 1)\n    \"\"\"\n    \n    W = np.random.randn(n_y, n_x) * 0.01\n    b = np.zeros((n_y, 1))\n\n    parameters = {\"W\": W,\n                  \"b\": b}\n    \n    return parameters\n\nparameters = initialize_parameters(n_x, n_y)\nprint(\"W = \" + str(parameters[\"W\"]))\nprint(\"b = \" + str(parameters[\"b\"]))\n\nW = [[-0.00768836 -0.00230031]]\nb = [[0.]]\n\n\nImplement forward_propagation() following the equation \\((4)\\) in the section 2.1: \\[\\begin{align}\nZ &=  W X + b,\\\\\nA &= \\sigma\\left(Z\\right).\n\\end{align}\\]\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A -- The output\n    \"\"\"\n    W = parameters[\"W\"]\n    b = parameters[\"b\"]\n    \n    # Forward Propagation to calculate Z.\n    Z = W @ X + b\n    A = sigmoid(Z)\n\n    return A\n\nA = forward_propagation(X, parameters)\n\nprint(\"Output vector A:\", A)\n\nOutput vector A: [[0.5        0.49942492 0.49807792 0.49750285 0.49942492 0.5\n  0.49942492 0.49807792 0.49807792 0.49750285 0.49942492 0.49807792\n  0.49807792 0.49750285 0.5        0.49750285 0.49807792 0.49942492\n  0.49942492 0.49942492 0.49942492 0.49807792 0.49750285 0.5\n  0.5        0.49942492 0.49750285 0.49942492 0.5        0.5       ]]\n\n\nYour weights were just initialized with some random values, so the model has not been trained yet.\nDefine a cost function \\((5)\\) which will be used to train the model:\n\\[\\mathcal{L}\\left(W, b\\right)  = \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small -y^{(i)}\\log\\left(a^{(i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{(i)}\\right)  \\large  \\right) \\small.\\]\n\ndef compute_cost(A, Y):\n    \"\"\"\n    Computes the log loss cost function\n    \n    Arguments:\n    A -- The output of the neural network of shape (n_y, number of examples)\n    Y -- \"true\" labels vector of shape (n_y, number of examples)\n    \n    Returns:\n    cost -- log loss\n    \n    \"\"\"\n    # Number of examples.\n    m = Y.shape[1]\n\n    # Compute the cost function.\n    logprobs = - (Y * np.log(A)) - (1 - Y) * np.log(1 - A)\n    cost = 1/m * np.sum(logprobs)\n    \n    return cost\n\nprint(\"cost = \" + str(compute_cost(A, Y)))\n\ncost = 0.6916391611507908\n\n\nCalculate partial derivatives as shown in \\((8)\\):\n\\[\\begin{align}\n\\frac{\\partial \\mathcal{L} }{ \\partial W } &= \\frac{1}{m}\\left(A - Y\\right)X^T,\\\\\n\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \\frac{1}{m}\\left(A - Y\\right)\\mathbf{1}.\n\\end{align}\\]\n\ndef backward_propagation(A, X, Y):\n    \"\"\"\n    Implements the backward propagation, calculating gradients\n    \n    Arguments:\n    A -- the output of the neural network of shape (n_y, number of examples)\n    X -- input data of shape (n_x, number of examples)\n    Y -- \"true\" labels vector of shape (n_y, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # Backward propagation: calculate partial derivatives denoted as dW, db for simplicity. \n    dZ = A - Y\n    dW = 1/m * (dZ @ X.T)\n    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n    \n    grads = {\"dW\": dW,\n             \"db\": db}\n    \n    return grads\n\ngrads = backward_propagation(A, X, Y)\n\nprint(\"dW = \" + str(grads[\"dW\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n\ndW = [[ 0.21571875 -0.06735779]]\ndb = [[0.16552706]]\n\n\nUpdate parameters as shown in \\((9)\\):\n\\[\\begin{align}\nW &= W - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W },\\\\\nb &= b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b }.\\end{align}\\]\n\ndef update_parameters(parameters, grads, learning_rate=1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule\n    \n    Arguments:\n    parameters -- python dictionary containing parameters \n    grads -- python dictionary containing gradients \n    learning_rate -- learning rate parameter for gradient descent\n    \n    Returns:\n    parameters -- python dictionary containing updated parameters \n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\".\n    W = parameters[\"W\"]\n    b = parameters[\"b\"]\n    \n    # Retrieve each gradient from the dictionary \"grads\".\n    dW = grads[\"dW\"]\n    db = grads[\"db\"]\n    \n    # Update rule for each parameter.\n    W = W - learning_rate * dW\n    b = b - learning_rate * db\n    \n    parameters = {\"W\": W,\n                  \"b\": b}\n    \n    return parameters\n\nparameters_updated = update_parameters(parameters, grads)\n\nprint(\"W updated = \" + str(parameters_updated[\"W\"]))\nprint(\"b updated = \" + str(parameters_updated[\"b\"]))\n\nW updated = [[-0.26655087  0.07852904]]\nb updated = [[-0.19863247]]\n\n\nBuild your neural network model in nn_model().\n\ndef nn_model(X, Y, num_iterations=10, learning_rate=1.2, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (n_x, number of examples)\n    Y -- labels of shape (n_y, number of examples)\n    num_iterations -- number of iterations in the loop\n    learning_rate -- learning rate parameter for gradient descent\n    print_cost -- if True, print the cost every iteration\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to make predictions.\n    \"\"\"\n    \n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[1]\n    \n    parameters = initialize_parameters(n_x, n_y)\n    \n    # Loop\n    for i in range(0, num_iterations):\n         \n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A\".\n        A = forward_propagation(X, parameters)\n        \n        # Cost function. Inputs: \"A, Y\". Outputs: \"cost\".\n        cost = compute_cost(A, Y)\n        \n        # Backpropagation. Inputs: \"A, X, Y\". Outputs: \"grads\".\n        grads = backward_propagation(A, X, Y)\n    \n        # Gradient descent parameter update. Inputs: \"parameters, grads, learning_rate\". Outputs: \"parameters\".\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the cost every iteration.\n        if print_cost:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters\n\n\nparameters = nn_model(X, Y, num_iterations=500, learning_rate=1.2, print_cost=True)\nprint(\"W = \" + str(parameters[\"W\"]))\nprint(\"b = \" + str(parameters[\"b\"]))\n\nCost after iteration 0: 0.693480\nCost after iteration 1: 0.608586\nCost after iteration 2: 0.554475\nCost after iteration 3: 0.513124\nCost after iteration 4: 0.478828\nCost after iteration 5: 0.449395\nCost after iteration 6: 0.423719\nCost after iteration 7: 0.401089\nCost after iteration 8: 0.380986\nCost after iteration 9: 0.363002\nCost after iteration 10: 0.346813\nCost after iteration 11: 0.332152\nCost after iteration 12: 0.318805\nCost after iteration 13: 0.306594\nCost after iteration 14: 0.295369\nCost after iteration 15: 0.285010\nCost after iteration 16: 0.275412\nCost after iteration 17: 0.266489\nCost after iteration 18: 0.258167\nCost after iteration 19: 0.250382\nCost after iteration 20: 0.243080\nCost after iteration 21: 0.236215\nCost after iteration 22: 0.229745\nCost after iteration 23: 0.223634\nCost after iteration 24: 0.217853\nCost after iteration 25: 0.212372\nCost after iteration 26: 0.207168\nCost after iteration 27: 0.202219\nCost after iteration 28: 0.197505\nCost after iteration 29: 0.193009\nCost after iteration 30: 0.188716\nCost after iteration 31: 0.184611\nCost after iteration 32: 0.180682\nCost after iteration 33: 0.176917\nCost after iteration 34: 0.173306\nCost after iteration 35: 0.169839\nCost after iteration 36: 0.166507\nCost after iteration 37: 0.163303\nCost after iteration 38: 0.160218\nCost after iteration 39: 0.157246\nCost after iteration 40: 0.154382\nCost after iteration 41: 0.151618\nCost after iteration 42: 0.148950\nCost after iteration 43: 0.146373\nCost after iteration 44: 0.143881\nCost after iteration 45: 0.141471\nCost after iteration 46: 0.139139\nCost after iteration 47: 0.136881\nCost after iteration 48: 0.134694\nCost after iteration 49: 0.132574\nCost after iteration 50: 0.130517\nCost after iteration 51: 0.128522\nCost after iteration 52: 0.126586\nCost after iteration 53: 0.124705\nCost after iteration 54: 0.122878\nCost after iteration 55: 0.121102\nCost after iteration 56: 0.119375\nCost after iteration 57: 0.117696\nCost after iteration 58: 0.116062\nCost after iteration 59: 0.114471\nCost after iteration 60: 0.112922\nCost after iteration 61: 0.111413\nCost after iteration 62: 0.109942\nCost after iteration 63: 0.108509\nCost after iteration 64: 0.107111\nCost after iteration 65: 0.105748\nCost after iteration 66: 0.104418\nCost after iteration 67: 0.103120\nCost after iteration 68: 0.101853\nCost after iteration 69: 0.100616\nCost after iteration 70: 0.099407\nCost after iteration 71: 0.098227\nCost after iteration 72: 0.097073\nCost after iteration 73: 0.095945\nCost after iteration 74: 0.094842\nCost after iteration 75: 0.093763\nCost after iteration 76: 0.092708\nCost after iteration 77: 0.091675\nCost after iteration 78: 0.090665\nCost after iteration 79: 0.089676\nCost after iteration 80: 0.088707\nCost after iteration 81: 0.087759\nCost after iteration 82: 0.086830\nCost after iteration 83: 0.085920\nCost after iteration 84: 0.085028\nCost after iteration 85: 0.084154\nCost after iteration 86: 0.083297\nCost after iteration 87: 0.082457\nCost after iteration 88: 0.081633\nCost after iteration 89: 0.080824\nCost after iteration 90: 0.080032\nCost after iteration 91: 0.079254\nCost after iteration 92: 0.078490\nCost after iteration 93: 0.077741\nCost after iteration 94: 0.077005\nCost after iteration 95: 0.076283\nCost after iteration 96: 0.075574\nCost after iteration 97: 0.074877\nCost after iteration 98: 0.074193\nCost after iteration 99: 0.073521\nCost after iteration 100: 0.072860\nCost after iteration 101: 0.072211\nCost after iteration 102: 0.071573\nCost after iteration 103: 0.070946\nCost after iteration 104: 0.070330\nCost after iteration 105: 0.069723\nCost after iteration 106: 0.069127\nCost after iteration 107: 0.068541\nCost after iteration 108: 0.067964\nCost after iteration 109: 0.067396\nCost after iteration 110: 0.066838\nCost after iteration 111: 0.066288\nCost after iteration 112: 0.065748\nCost after iteration 113: 0.065215\nCost after iteration 114: 0.064691\nCost after iteration 115: 0.064175\nCost after iteration 116: 0.063667\nCost after iteration 117: 0.063167\nCost after iteration 118: 0.062675\nCost after iteration 119: 0.062189\nCost after iteration 120: 0.061711\nCost after iteration 121: 0.061240\nCost after iteration 122: 0.060777\nCost after iteration 123: 0.060319\nCost after iteration 124: 0.059869\nCost after iteration 125: 0.059425\nCost after iteration 126: 0.058987\nCost after iteration 127: 0.058556\nCost after iteration 128: 0.058130\nCost after iteration 129: 0.057711\nCost after iteration 130: 0.057297\nCost after iteration 131: 0.056889\nCost after iteration 132: 0.056487\nCost after iteration 133: 0.056091\nCost after iteration 134: 0.055699\nCost after iteration 135: 0.055313\nCost after iteration 136: 0.054932\nCost after iteration 137: 0.054556\nCost after iteration 138: 0.054186\nCost after iteration 139: 0.053820\nCost after iteration 140: 0.053458\nCost after iteration 141: 0.053102\nCost after iteration 142: 0.052750\nCost after iteration 143: 0.052403\nCost after iteration 144: 0.052060\nCost after iteration 145: 0.051721\nCost after iteration 146: 0.051387\nCost after iteration 147: 0.051057\nCost after iteration 148: 0.050731\nCost after iteration 149: 0.050409\nCost after iteration 150: 0.050091\nCost after iteration 151: 0.049776\nCost after iteration 152: 0.049466\nCost after iteration 153: 0.049160\nCost after iteration 154: 0.048857\nCost after iteration 155: 0.048557\nCost after iteration 156: 0.048262\nCost after iteration 157: 0.047969\nCost after iteration 158: 0.047681\nCost after iteration 159: 0.047395\nCost after iteration 160: 0.047113\nCost after iteration 161: 0.046834\nCost after iteration 162: 0.046559\nCost after iteration 163: 0.046286\nCost after iteration 164: 0.046017\nCost after iteration 165: 0.045750\nCost after iteration 166: 0.045487\nCost after iteration 167: 0.045227\nCost after iteration 168: 0.044969\nCost after iteration 169: 0.044714\nCost after iteration 170: 0.044463\nCost after iteration 171: 0.044213\nCost after iteration 172: 0.043967\nCost after iteration 173: 0.043723\nCost after iteration 174: 0.043482\nCost after iteration 175: 0.043244\nCost after iteration 176: 0.043008\nCost after iteration 177: 0.042774\nCost after iteration 178: 0.042543\nCost after iteration 179: 0.042315\nCost after iteration 180: 0.042089\nCost after iteration 181: 0.041865\nCost after iteration 182: 0.041643\nCost after iteration 183: 0.041424\nCost after iteration 184: 0.041207\nCost after iteration 185: 0.040992\nCost after iteration 186: 0.040780\nCost after iteration 187: 0.040569\nCost after iteration 188: 0.040361\nCost after iteration 189: 0.040155\nCost after iteration 190: 0.039950\nCost after iteration 191: 0.039748\nCost after iteration 192: 0.039548\nCost after iteration 193: 0.039350\nCost after iteration 194: 0.039154\nCost after iteration 195: 0.038959\nCost after iteration 196: 0.038767\nCost after iteration 197: 0.038576\nCost after iteration 198: 0.038387\nCost after iteration 199: 0.038200\nCost after iteration 200: 0.038015\nCost after iteration 201: 0.037832\nCost after iteration 202: 0.037650\nCost after iteration 203: 0.037470\nCost after iteration 204: 0.037291\nCost after iteration 205: 0.037115\nCost after iteration 206: 0.036940\nCost after iteration 207: 0.036766\nCost after iteration 208: 0.036594\nCost after iteration 209: 0.036424\nCost after iteration 210: 0.036255\nCost after iteration 211: 0.036088\nCost after iteration 212: 0.035922\nCost after iteration 213: 0.035758\nCost after iteration 214: 0.035595\nCost after iteration 215: 0.035434\nCost after iteration 216: 0.035274\nCost after iteration 217: 0.035116\nCost after iteration 218: 0.034958\nCost after iteration 219: 0.034803\nCost after iteration 220: 0.034648\nCost after iteration 221: 0.034495\nCost after iteration 222: 0.034344\nCost after iteration 223: 0.034193\nCost after iteration 224: 0.034044\nCost after iteration 225: 0.033896\nCost after iteration 226: 0.033750\nCost after iteration 227: 0.033604\nCost after iteration 228: 0.033460\nCost after iteration 229: 0.033317\nCost after iteration 230: 0.033176\nCost after iteration 231: 0.033035\nCost after iteration 232: 0.032896\nCost after iteration 233: 0.032757\nCost after iteration 234: 0.032620\nCost after iteration 235: 0.032484\nCost after iteration 236: 0.032349\nCost after iteration 237: 0.032216\nCost after iteration 238: 0.032083\nCost after iteration 239: 0.031951\nCost after iteration 240: 0.031821\nCost after iteration 241: 0.031691\nCost after iteration 242: 0.031563\nCost after iteration 243: 0.031435\nCost after iteration 244: 0.031309\nCost after iteration 245: 0.031183\nCost after iteration 246: 0.031059\nCost after iteration 247: 0.030935\nCost after iteration 248: 0.030813\nCost after iteration 249: 0.030691\nCost after iteration 250: 0.030571\nCost after iteration 251: 0.030451\nCost after iteration 252: 0.030332\nCost after iteration 253: 0.030214\nCost after iteration 254: 0.030097\nCost after iteration 255: 0.029981\nCost after iteration 256: 0.029866\nCost after iteration 257: 0.029752\nCost after iteration 258: 0.029638\nCost after iteration 259: 0.029525\nCost after iteration 260: 0.029414\nCost after iteration 261: 0.029303\nCost after iteration 262: 0.029192\nCost after iteration 263: 0.029083\nCost after iteration 264: 0.028974\nCost after iteration 265: 0.028867\nCost after iteration 266: 0.028760\nCost after iteration 267: 0.028654\nCost after iteration 268: 0.028548\nCost after iteration 269: 0.028443\nCost after iteration 270: 0.028340\nCost after iteration 271: 0.028236\nCost after iteration 272: 0.028134\nCost after iteration 273: 0.028032\nCost after iteration 274: 0.027931\nCost after iteration 275: 0.027831\nCost after iteration 276: 0.027731\nCost after iteration 277: 0.027633\nCost after iteration 278: 0.027534\nCost after iteration 279: 0.027437\nCost after iteration 280: 0.027340\nCost after iteration 281: 0.027244\nCost after iteration 282: 0.027148\nCost after iteration 283: 0.027054\nCost after iteration 284: 0.026959\nCost after iteration 285: 0.026866\nCost after iteration 286: 0.026773\nCost after iteration 287: 0.026681\nCost after iteration 288: 0.026589\nCost after iteration 289: 0.026498\nCost after iteration 290: 0.026408\nCost after iteration 291: 0.026318\nCost after iteration 292: 0.026229\nCost after iteration 293: 0.026140\nCost after iteration 294: 0.026052\nCost after iteration 295: 0.025965\nCost after iteration 296: 0.025878\nCost after iteration 297: 0.025792\nCost after iteration 298: 0.025706\nCost after iteration 299: 0.025621\nCost after iteration 300: 0.025536\nCost after iteration 301: 0.025452\nCost after iteration 302: 0.025368\nCost after iteration 303: 0.025285\nCost after iteration 304: 0.025203\nCost after iteration 305: 0.025121\nCost after iteration 306: 0.025040\nCost after iteration 307: 0.024959\nCost after iteration 308: 0.024878\nCost after iteration 309: 0.024799\nCost after iteration 310: 0.024719\nCost after iteration 311: 0.024640\nCost after iteration 312: 0.024562\nCost after iteration 313: 0.024484\nCost after iteration 314: 0.024407\nCost after iteration 315: 0.024330\nCost after iteration 316: 0.024253\nCost after iteration 317: 0.024178\nCost after iteration 318: 0.024102\nCost after iteration 319: 0.024027\nCost after iteration 320: 0.023952\nCost after iteration 321: 0.023878\nCost after iteration 322: 0.023805\nCost after iteration 323: 0.023732\nCost after iteration 324: 0.023659\nCost after iteration 325: 0.023586\nCost after iteration 326: 0.023515\nCost after iteration 327: 0.023443\nCost after iteration 328: 0.023372\nCost after iteration 329: 0.023301\nCost after iteration 330: 0.023231\nCost after iteration 331: 0.023161\nCost after iteration 332: 0.023092\nCost after iteration 333: 0.023023\nCost after iteration 334: 0.022955\nCost after iteration 335: 0.022886\nCost after iteration 336: 0.022819\nCost after iteration 337: 0.022751\nCost after iteration 338: 0.022684\nCost after iteration 339: 0.022618\nCost after iteration 340: 0.022552\nCost after iteration 341: 0.022486\nCost after iteration 342: 0.022420\nCost after iteration 343: 0.022355\nCost after iteration 344: 0.022291\nCost after iteration 345: 0.022226\nCost after iteration 346: 0.022162\nCost after iteration 347: 0.022099\nCost after iteration 348: 0.022036\nCost after iteration 349: 0.021973\nCost after iteration 350: 0.021910\nCost after iteration 351: 0.021848\nCost after iteration 352: 0.021786\nCost after iteration 353: 0.021725\nCost after iteration 354: 0.021664\nCost after iteration 355: 0.021603\nCost after iteration 356: 0.021542\nCost after iteration 357: 0.021482\nCost after iteration 358: 0.021422\nCost after iteration 359: 0.021363\nCost after iteration 360: 0.021304\nCost after iteration 361: 0.021245\nCost after iteration 362: 0.021186\nCost after iteration 363: 0.021128\nCost after iteration 364: 0.021070\nCost after iteration 365: 0.021013\nCost after iteration 366: 0.020956\nCost after iteration 367: 0.020899\nCost after iteration 368: 0.020842\nCost after iteration 369: 0.020786\nCost after iteration 370: 0.020730\nCost after iteration 371: 0.020674\nCost after iteration 372: 0.020618\nCost after iteration 373: 0.020563\nCost after iteration 374: 0.020508\nCost after iteration 375: 0.020454\nCost after iteration 376: 0.020400\nCost after iteration 377: 0.020346\nCost after iteration 378: 0.020292\nCost after iteration 379: 0.020238\nCost after iteration 380: 0.020185\nCost after iteration 381: 0.020132\nCost after iteration 382: 0.020080\nCost after iteration 383: 0.020028\nCost after iteration 384: 0.019975\nCost after iteration 385: 0.019924\nCost after iteration 386: 0.019872\nCost after iteration 387: 0.019821\nCost after iteration 388: 0.019770\nCost after iteration 389: 0.019719\nCost after iteration 390: 0.019669\nCost after iteration 391: 0.019618\nCost after iteration 392: 0.019568\nCost after iteration 393: 0.019519\nCost after iteration 394: 0.019469\nCost after iteration 395: 0.019420\nCost after iteration 396: 0.019371\nCost after iteration 397: 0.019322\nCost after iteration 398: 0.019274\nCost after iteration 399: 0.019225\nCost after iteration 400: 0.019177\nCost after iteration 401: 0.019129\nCost after iteration 402: 0.019082\nCost after iteration 403: 0.019035\nCost after iteration 404: 0.018988\nCost after iteration 405: 0.018941\nCost after iteration 406: 0.018894\nCost after iteration 407: 0.018848\nCost after iteration 408: 0.018801\nCost after iteration 409: 0.018755\nCost after iteration 410: 0.018710\nCost after iteration 411: 0.018664\nCost after iteration 412: 0.018619\nCost after iteration 413: 0.018574\nCost after iteration 414: 0.018529\nCost after iteration 415: 0.018484\nCost after iteration 416: 0.018440\nCost after iteration 417: 0.018396\nCost after iteration 418: 0.018352\nCost after iteration 419: 0.018308\nCost after iteration 420: 0.018264\nCost after iteration 421: 0.018221\nCost after iteration 422: 0.018178\nCost after iteration 423: 0.018135\nCost after iteration 424: 0.018092\nCost after iteration 425: 0.018049\nCost after iteration 426: 0.018007\nCost after iteration 427: 0.017965\nCost after iteration 428: 0.017923\nCost after iteration 429: 0.017881\nCost after iteration 430: 0.017839\nCost after iteration 431: 0.017798\nCost after iteration 432: 0.017756\nCost after iteration 433: 0.017715\nCost after iteration 434: 0.017675\nCost after iteration 435: 0.017634\nCost after iteration 436: 0.017593\nCost after iteration 437: 0.017553\nCost after iteration 438: 0.017513\nCost after iteration 439: 0.017473\nCost after iteration 440: 0.017433\nCost after iteration 441: 0.017394\nCost after iteration 442: 0.017354\nCost after iteration 443: 0.017315\nCost after iteration 444: 0.017276\nCost after iteration 445: 0.017237\nCost after iteration 446: 0.017198\nCost after iteration 447: 0.017160\nCost after iteration 448: 0.017121\nCost after iteration 449: 0.017083\nCost after iteration 450: 0.017045\nCost after iteration 451: 0.017007\nCost after iteration 452: 0.016970\nCost after iteration 453: 0.016932\nCost after iteration 454: 0.016895\nCost after iteration 455: 0.016858\nCost after iteration 456: 0.016821\nCost after iteration 457: 0.016784\nCost after iteration 458: 0.016747\nCost after iteration 459: 0.016710\nCost after iteration 460: 0.016674\nCost after iteration 461: 0.016638\nCost after iteration 462: 0.016602\nCost after iteration 463: 0.016566\nCost after iteration 464: 0.016530\nCost after iteration 465: 0.016494\nCost after iteration 466: 0.016459\nCost after iteration 467: 0.016423\nCost after iteration 468: 0.016388\nCost after iteration 469: 0.016353\nCost after iteration 470: 0.016318\nCost after iteration 471: 0.016284\nCost after iteration 472: 0.016249\nCost after iteration 473: 0.016215\nCost after iteration 474: 0.016180\nCost after iteration 475: 0.016146\nCost after iteration 476: 0.016112\nCost after iteration 477: 0.016078\nCost after iteration 478: 0.016045\nCost after iteration 479: 0.016011\nCost after iteration 480: 0.015977\nCost after iteration 481: 0.015944\nCost after iteration 482: 0.015911\nCost after iteration 483: 0.015878\nCost after iteration 484: 0.015845\nCost after iteration 485: 0.015812\nCost after iteration 486: 0.015780\nCost after iteration 487: 0.015747\nCost after iteration 488: 0.015715\nCost after iteration 489: 0.015683\nCost after iteration 490: 0.015650\nCost after iteration 491: 0.015618\nCost after iteration 492: 0.015587\nCost after iteration 493: 0.015555\nCost after iteration 494: 0.015523\nCost after iteration 495: 0.015492\nCost after iteration 496: 0.015460\nCost after iteration 497: 0.015429\nCost after iteration 498: 0.015398\nCost after iteration 499: 0.015367\nW = [[-7.94930555  7.68597905]]\nb = [[-3.80585979]]\n\n\n\ndef plot_decision_boundary(X, Y, parameters):\n    W = parameters[\"W\"]\n    b = parameters[\"b\"]\n\n    fig, ax = plt.subplots()\n    plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']));\n    \n    x_line = np.arange(np.min(X[0,:]),np.max(X[0,:])*1.1, 0.1)\n    ax.plot(x_line, - W[0,0] / W[0,1] * x_line + -b[0,0] / W[0,1] , color=\"black\")\n    plt.plot()\n    plt.show()\n    \nplot_decision_boundary(X, Y, parameters)\n\n\n\n\n\n\n\n\n\ndef predict(X, parameters):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (blue: False / red: True)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n    A = forward_propagation(X, parameters)\n    predictions = A &gt; 0.5\n    \n    return predictions\n\nX_pred = np.array([[1, 1, 0, 0],\n                   [0, 1, 0, 1]])\nY_pred = predict(X_pred, parameters)\n\nprint(f\"Coordinates (in the columns):\\n{X_pred}\")\nprint(f\"Predictions:\\n{Y_pred}\")\n\nCoordinates (in the columns):\n[[1 1 0 0]\n [0 1 0 1]]\nPredictions:\n[[False False False  True]]\n\n\n ## 3 - Performance on a Larger Dataset\nConstruct a larger and more complex dataset with the function make_blobs from the sklearn.datasets library:\n\n# Dataset\nn_samples = 1000\nsamples, labels = make_blobs(n_samples=n_samples, \n                             centers=([2.5, 3], [6.7, 7.9]), \n                             cluster_std=1.4,\n                             random_state=0)\n\nX_larger = np.transpose(samples)\nY_larger = labels.reshape((1,n_samples))\n\nplt.scatter(X_larger[0, :], X_larger[1, :], c=Y_larger, cmap=colors.ListedColormap(['blue', 'red']));\n\n\n\n\n\n\n\n\nAnd train your neural network for \\(600\\) iterations.\n\nparameters_larger = nn_model(X_larger, Y_larger, num_iterations=600, learning_rate=0.1, print_cost=True)\nprint(\"W = \" + str(parameters_larger[\"W\"]))\nprint(\"b = \" + str(parameters_larger[\"b\"]))\n\nCost after iteration 0: 0.715632\nCost after iteration 1: 0.619924\nCost after iteration 2: 0.598855\nCost after iteration 3: 0.595347\nCost after iteration 4: 0.591877\nCost after iteration 5: 0.588444\nCost after iteration 6: 0.585047\nCost after iteration 7: 0.581686\nCost after iteration 8: 0.578359\nCost after iteration 9: 0.575067\nCost after iteration 10: 0.571807\nCost after iteration 11: 0.568581\nCost after iteration 12: 0.565387\nCost after iteration 13: 0.562225\nCost after iteration 14: 0.559094\nCost after iteration 15: 0.555995\nCost after iteration 16: 0.552925\nCost after iteration 17: 0.549885\nCost after iteration 18: 0.546875\nCost after iteration 19: 0.543894\nCost after iteration 20: 0.540942\nCost after iteration 21: 0.538018\nCost after iteration 22: 0.535121\nCost after iteration 23: 0.532252\nCost after iteration 24: 0.529411\nCost after iteration 25: 0.526596\nCost after iteration 26: 0.523807\nCost after iteration 27: 0.521045\nCost after iteration 28: 0.518309\nCost after iteration 29: 0.515598\nCost after iteration 30: 0.512912\nCost after iteration 31: 0.510251\nCost after iteration 32: 0.507615\nCost after iteration 33: 0.505002\nCost after iteration 34: 0.502414\nCost after iteration 35: 0.499850\nCost after iteration 36: 0.497309\nCost after iteration 37: 0.494791\nCost after iteration 38: 0.492297\nCost after iteration 39: 0.489825\nCost after iteration 40: 0.487375\nCost after iteration 41: 0.484947\nCost after iteration 42: 0.482542\nCost after iteration 43: 0.480158\nCost after iteration 44: 0.477795\nCost after iteration 45: 0.475454\nCost after iteration 46: 0.473134\nCost after iteration 47: 0.470834\nCost after iteration 48: 0.468555\nCost after iteration 49: 0.466296\nCost after iteration 50: 0.464057\nCost after iteration 51: 0.461838\nCost after iteration 52: 0.459639\nCost after iteration 53: 0.457459\nCost after iteration 54: 0.455299\nCost after iteration 55: 0.453157\nCost after iteration 56: 0.451034\nCost after iteration 57: 0.448930\nCost after iteration 58: 0.446844\nCost after iteration 59: 0.444776\nCost after iteration 60: 0.442727\nCost after iteration 61: 0.440695\nCost after iteration 62: 0.438681\nCost after iteration 63: 0.436684\nCost after iteration 64: 0.434705\nCost after iteration 65: 0.432742\nCost after iteration 66: 0.430796\nCost after iteration 67: 0.428868\nCost after iteration 68: 0.426955\nCost after iteration 69: 0.425059\nCost after iteration 70: 0.423179\nCost after iteration 71: 0.421316\nCost after iteration 72: 0.419467\nCost after iteration 73: 0.417635\nCost after iteration 74: 0.415818\nCost after iteration 75: 0.414017\nCost after iteration 76: 0.412230\nCost after iteration 77: 0.410459\nCost after iteration 78: 0.408702\nCost after iteration 79: 0.406961\nCost after iteration 80: 0.405233\nCost after iteration 81: 0.403520\nCost after iteration 82: 0.401822\nCost after iteration 83: 0.400137\nCost after iteration 84: 0.398466\nCost after iteration 85: 0.396809\nCost after iteration 86: 0.395166\nCost after iteration 87: 0.393536\nCost after iteration 88: 0.391920\nCost after iteration 89: 0.390316\nCost after iteration 90: 0.388726\nCost after iteration 91: 0.387149\nCost after iteration 92: 0.385585\nCost after iteration 93: 0.384033\nCost after iteration 94: 0.382493\nCost after iteration 95: 0.380966\nCost after iteration 96: 0.379452\nCost after iteration 97: 0.377949\nCost after iteration 98: 0.376459\nCost after iteration 99: 0.374980\nCost after iteration 100: 0.373513\nCost after iteration 101: 0.372058\nCost after iteration 102: 0.370614\nCost after iteration 103: 0.369182\nCost after iteration 104: 0.367761\nCost after iteration 105: 0.366351\nCost after iteration 106: 0.364952\nCost after iteration 107: 0.363564\nCost after iteration 108: 0.362187\nCost after iteration 109: 0.360821\nCost after iteration 110: 0.359465\nCost after iteration 111: 0.358120\nCost after iteration 112: 0.356785\nCost after iteration 113: 0.355460\nCost after iteration 114: 0.354145\nCost after iteration 115: 0.352841\nCost after iteration 116: 0.351546\nCost after iteration 117: 0.350262\nCost after iteration 118: 0.348987\nCost after iteration 119: 0.347722\nCost after iteration 120: 0.346466\nCost after iteration 121: 0.345220\nCost after iteration 122: 0.343983\nCost after iteration 123: 0.342755\nCost after iteration 124: 0.341537\nCost after iteration 125: 0.340327\nCost after iteration 126: 0.339127\nCost after iteration 127: 0.337936\nCost after iteration 128: 0.336753\nCost after iteration 129: 0.335579\nCost after iteration 130: 0.334414\nCost after iteration 131: 0.333257\nCost after iteration 132: 0.332109\nCost after iteration 133: 0.330969\nCost after iteration 134: 0.329837\nCost after iteration 135: 0.328714\nCost after iteration 136: 0.327598\nCost after iteration 137: 0.326491\nCost after iteration 138: 0.325392\nCost after iteration 139: 0.324300\nCost after iteration 140: 0.323217\nCost after iteration 141: 0.322141\nCost after iteration 142: 0.321073\nCost after iteration 143: 0.320012\nCost after iteration 144: 0.318959\nCost after iteration 145: 0.317913\nCost after iteration 146: 0.316875\nCost after iteration 147: 0.315844\nCost after iteration 148: 0.314820\nCost after iteration 149: 0.313804\nCost after iteration 150: 0.312794\nCost after iteration 151: 0.311792\nCost after iteration 152: 0.310796\nCost after iteration 153: 0.309807\nCost after iteration 154: 0.308825\nCost after iteration 155: 0.307850\nCost after iteration 156: 0.306882\nCost after iteration 157: 0.305920\nCost after iteration 158: 0.304965\nCost after iteration 159: 0.304016\nCost after iteration 160: 0.303073\nCost after iteration 161: 0.302137\nCost after iteration 162: 0.301208\nCost after iteration 163: 0.300284\nCost after iteration 164: 0.299367\nCost after iteration 165: 0.298456\nCost after iteration 166: 0.297551\nCost after iteration 167: 0.296652\nCost after iteration 168: 0.295759\nCost after iteration 169: 0.294872\nCost after iteration 170: 0.293990\nCost after iteration 171: 0.293115\nCost after iteration 172: 0.292245\nCost after iteration 173: 0.291381\nCost after iteration 174: 0.290522\nCost after iteration 175: 0.289670\nCost after iteration 176: 0.288822\nCost after iteration 177: 0.287980\nCost after iteration 178: 0.287144\nCost after iteration 179: 0.286313\nCost after iteration 180: 0.285487\nCost after iteration 181: 0.284667\nCost after iteration 182: 0.283852\nCost after iteration 183: 0.283042\nCost after iteration 184: 0.282237\nCost after iteration 185: 0.281437\nCost after iteration 186: 0.280643\nCost after iteration 187: 0.279853\nCost after iteration 188: 0.279068\nCost after iteration 189: 0.278288\nCost after iteration 190: 0.277513\nCost after iteration 191: 0.276743\nCost after iteration 192: 0.275978\nCost after iteration 193: 0.275218\nCost after iteration 194: 0.274462\nCost after iteration 195: 0.273710\nCost after iteration 196: 0.272964\nCost after iteration 197: 0.272222\nCost after iteration 198: 0.271484\nCost after iteration 199: 0.270752\nCost after iteration 200: 0.270023\nCost after iteration 201: 0.269299\nCost after iteration 202: 0.268579\nCost after iteration 203: 0.267864\nCost after iteration 204: 0.267153\nCost after iteration 205: 0.266446\nCost after iteration 206: 0.265744\nCost after iteration 207: 0.265045\nCost after iteration 208: 0.264351\nCost after iteration 209: 0.263661\nCost after iteration 210: 0.262975\nCost after iteration 211: 0.262293\nCost after iteration 212: 0.261615\nCost after iteration 213: 0.260941\nCost after iteration 214: 0.260271\nCost after iteration 215: 0.259605\nCost after iteration 216: 0.258943\nCost after iteration 217: 0.258285\nCost after iteration 218: 0.257630\nCost after iteration 219: 0.256980\nCost after iteration 220: 0.256333\nCost after iteration 221: 0.255689\nCost after iteration 222: 0.255050\nCost after iteration 223: 0.254414\nCost after iteration 224: 0.253782\nCost after iteration 225: 0.253153\nCost after iteration 226: 0.252528\nCost after iteration 227: 0.251907\nCost after iteration 228: 0.251289\nCost after iteration 229: 0.250674\nCost after iteration 230: 0.250063\nCost after iteration 231: 0.249455\nCost after iteration 232: 0.248851\nCost after iteration 233: 0.248250\nCost after iteration 234: 0.247653\nCost after iteration 235: 0.247059\nCost after iteration 236: 0.246468\nCost after iteration 237: 0.245880\nCost after iteration 238: 0.245296\nCost after iteration 239: 0.244714\nCost after iteration 240: 0.244136\nCost after iteration 241: 0.243562\nCost after iteration 242: 0.242990\nCost after iteration 243: 0.242421\nCost after iteration 244: 0.241856\nCost after iteration 245: 0.241293\nCost after iteration 246: 0.240734\nCost after iteration 247: 0.240178\nCost after iteration 248: 0.239624\nCost after iteration 249: 0.239074\nCost after iteration 250: 0.238526\nCost after iteration 251: 0.237982\nCost after iteration 252: 0.237440\nCost after iteration 253: 0.236901\nCost after iteration 254: 0.236365\nCost after iteration 255: 0.235832\nCost after iteration 256: 0.235302\nCost after iteration 257: 0.234774\nCost after iteration 258: 0.234250\nCost after iteration 259: 0.233728\nCost after iteration 260: 0.233208\nCost after iteration 261: 0.232692\nCost after iteration 262: 0.232178\nCost after iteration 263: 0.231667\nCost after iteration 264: 0.231158\nCost after iteration 265: 0.230652\nCost after iteration 266: 0.230149\nCost after iteration 267: 0.229648\nCost after iteration 268: 0.229150\nCost after iteration 269: 0.228654\nCost after iteration 270: 0.228161\nCost after iteration 271: 0.227670\nCost after iteration 272: 0.227182\nCost after iteration 273: 0.226696\nCost after iteration 274: 0.226213\nCost after iteration 275: 0.225732\nCost after iteration 276: 0.225254\nCost after iteration 277: 0.224778\nCost after iteration 278: 0.224304\nCost after iteration 279: 0.223833\nCost after iteration 280: 0.223364\nCost after iteration 281: 0.222897\nCost after iteration 282: 0.222433\nCost after iteration 283: 0.221971\nCost after iteration 284: 0.221511\nCost after iteration 285: 0.221054\nCost after iteration 286: 0.220598\nCost after iteration 287: 0.220145\nCost after iteration 288: 0.219695\nCost after iteration 289: 0.219246\nCost after iteration 290: 0.218799\nCost after iteration 291: 0.218355\nCost after iteration 292: 0.217913\nCost after iteration 293: 0.217473\nCost after iteration 294: 0.217035\nCost after iteration 295: 0.216599\nCost after iteration 296: 0.216166\nCost after iteration 297: 0.215734\nCost after iteration 298: 0.215304\nCost after iteration 299: 0.214877\nCost after iteration 300: 0.214451\nCost after iteration 301: 0.214028\nCost after iteration 302: 0.213606\nCost after iteration 303: 0.213187\nCost after iteration 304: 0.212769\nCost after iteration 305: 0.212354\nCost after iteration 306: 0.211940\nCost after iteration 307: 0.211528\nCost after iteration 308: 0.211119\nCost after iteration 309: 0.210711\nCost after iteration 310: 0.210305\nCost after iteration 311: 0.209901\nCost after iteration 312: 0.209498\nCost after iteration 313: 0.209098\nCost after iteration 314: 0.208699\nCost after iteration 315: 0.208303\nCost after iteration 316: 0.207908\nCost after iteration 317: 0.207515\nCost after iteration 318: 0.207123\nCost after iteration 319: 0.206734\nCost after iteration 320: 0.206346\nCost after iteration 321: 0.205960\nCost after iteration 322: 0.205575\nCost after iteration 323: 0.205193\nCost after iteration 324: 0.204812\nCost after iteration 325: 0.204433\nCost after iteration 326: 0.204055\nCost after iteration 327: 0.203680\nCost after iteration 328: 0.203306\nCost after iteration 329: 0.202933\nCost after iteration 330: 0.202562\nCost after iteration 331: 0.202193\nCost after iteration 332: 0.201826\nCost after iteration 333: 0.201460\nCost after iteration 334: 0.201095\nCost after iteration 335: 0.200733\nCost after iteration 336: 0.200372\nCost after iteration 337: 0.200012\nCost after iteration 338: 0.199654\nCost after iteration 339: 0.199298\nCost after iteration 340: 0.198943\nCost after iteration 341: 0.198590\nCost after iteration 342: 0.198238\nCost after iteration 343: 0.197887\nCost after iteration 344: 0.197539\nCost after iteration 345: 0.197191\nCost after iteration 346: 0.196846\nCost after iteration 347: 0.196501\nCost after iteration 348: 0.196158\nCost after iteration 349: 0.195817\nCost after iteration 350: 0.195477\nCost after iteration 351: 0.195138\nCost after iteration 352: 0.194801\nCost after iteration 353: 0.194466\nCost after iteration 354: 0.194131\nCost after iteration 355: 0.193799\nCost after iteration 356: 0.193467\nCost after iteration 357: 0.193137\nCost after iteration 358: 0.192808\nCost after iteration 359: 0.192481\nCost after iteration 360: 0.192155\nCost after iteration 361: 0.191830\nCost after iteration 362: 0.191507\nCost after iteration 363: 0.191185\nCost after iteration 364: 0.190865\nCost after iteration 365: 0.190545\nCost after iteration 366: 0.190227\nCost after iteration 367: 0.189911\nCost after iteration 368: 0.189595\nCost after iteration 369: 0.189281\nCost after iteration 370: 0.188968\nCost after iteration 371: 0.188657\nCost after iteration 372: 0.188346\nCost after iteration 373: 0.188037\nCost after iteration 374: 0.187730\nCost after iteration 375: 0.187423\nCost after iteration 376: 0.187118\nCost after iteration 377: 0.186814\nCost after iteration 378: 0.186511\nCost after iteration 379: 0.186209\nCost after iteration 380: 0.185909\nCost after iteration 381: 0.185609\nCost after iteration 382: 0.185311\nCost after iteration 383: 0.185014\nCost after iteration 384: 0.184719\nCost after iteration 385: 0.184424\nCost after iteration 386: 0.184131\nCost after iteration 387: 0.183839\nCost after iteration 388: 0.183547\nCost after iteration 389: 0.183257\nCost after iteration 390: 0.182969\nCost after iteration 391: 0.182681\nCost after iteration 392: 0.182394\nCost after iteration 393: 0.182109\nCost after iteration 394: 0.181824\nCost after iteration 395: 0.181541\nCost after iteration 396: 0.181259\nCost after iteration 397: 0.180978\nCost after iteration 398: 0.180698\nCost after iteration 399: 0.180419\nCost after iteration 400: 0.180141\nCost after iteration 401: 0.179864\nCost after iteration 402: 0.179589\nCost after iteration 403: 0.179314\nCost after iteration 404: 0.179040\nCost after iteration 405: 0.178768\nCost after iteration 406: 0.178496\nCost after iteration 407: 0.178226\nCost after iteration 408: 0.177956\nCost after iteration 409: 0.177688\nCost after iteration 410: 0.177420\nCost after iteration 411: 0.177153\nCost after iteration 412: 0.176888\nCost after iteration 413: 0.176623\nCost after iteration 414: 0.176360\nCost after iteration 415: 0.176097\nCost after iteration 416: 0.175836\nCost after iteration 417: 0.175575\nCost after iteration 418: 0.175316\nCost after iteration 419: 0.175057\nCost after iteration 420: 0.174799\nCost after iteration 421: 0.174542\nCost after iteration 422: 0.174286\nCost after iteration 423: 0.174032\nCost after iteration 424: 0.173778\nCost after iteration 425: 0.173525\nCost after iteration 426: 0.173272\nCost after iteration 427: 0.173021\nCost after iteration 428: 0.172771\nCost after iteration 429: 0.172522\nCost after iteration 430: 0.172273\nCost after iteration 431: 0.172025\nCost after iteration 432: 0.171779\nCost after iteration 433: 0.171533\nCost after iteration 434: 0.171288\nCost after iteration 435: 0.171044\nCost after iteration 436: 0.170801\nCost after iteration 437: 0.170558\nCost after iteration 438: 0.170317\nCost after iteration 439: 0.170076\nCost after iteration 440: 0.169837\nCost after iteration 441: 0.169598\nCost after iteration 442: 0.169360\nCost after iteration 443: 0.169123\nCost after iteration 444: 0.168886\nCost after iteration 445: 0.168651\nCost after iteration 446: 0.168416\nCost after iteration 447: 0.168182\nCost after iteration 448: 0.167949\nCost after iteration 449: 0.167717\nCost after iteration 450: 0.167485\nCost after iteration 451: 0.167255\nCost after iteration 452: 0.167025\nCost after iteration 453: 0.166796\nCost after iteration 454: 0.166567\nCost after iteration 455: 0.166340\nCost after iteration 456: 0.166113\nCost after iteration 457: 0.165887\nCost after iteration 458: 0.165662\nCost after iteration 459: 0.165438\nCost after iteration 460: 0.165214\nCost after iteration 461: 0.164991\nCost after iteration 462: 0.164769\nCost after iteration 463: 0.164548\nCost after iteration 464: 0.164327\nCost after iteration 465: 0.164108\nCost after iteration 466: 0.163889\nCost after iteration 467: 0.163670\nCost after iteration 468: 0.163453\nCost after iteration 469: 0.163236\nCost after iteration 470: 0.163020\nCost after iteration 471: 0.162804\nCost after iteration 472: 0.162590\nCost after iteration 473: 0.162376\nCost after iteration 474: 0.162162\nCost after iteration 475: 0.161950\nCost after iteration 476: 0.161738\nCost after iteration 477: 0.161527\nCost after iteration 478: 0.161316\nCost after iteration 479: 0.161107\nCost after iteration 480: 0.160898\nCost after iteration 481: 0.160689\nCost after iteration 482: 0.160482\nCost after iteration 483: 0.160275\nCost after iteration 484: 0.160068\nCost after iteration 485: 0.159863\nCost after iteration 486: 0.159658\nCost after iteration 487: 0.159453\nCost after iteration 488: 0.159250\nCost after iteration 489: 0.159047\nCost after iteration 490: 0.158845\nCost after iteration 491: 0.158643\nCost after iteration 492: 0.158442\nCost after iteration 493: 0.158242\nCost after iteration 494: 0.158042\nCost after iteration 495: 0.157843\nCost after iteration 496: 0.157644\nCost after iteration 497: 0.157447\nCost after iteration 498: 0.157249\nCost after iteration 499: 0.157053\nCost after iteration 500: 0.156857\nCost after iteration 501: 0.156662\nCost after iteration 502: 0.156467\nCost after iteration 503: 0.156273\nCost after iteration 504: 0.156080\nCost after iteration 505: 0.155887\nCost after iteration 506: 0.155695\nCost after iteration 507: 0.155503\nCost after iteration 508: 0.155312\nCost after iteration 509: 0.155122\nCost after iteration 510: 0.154932\nCost after iteration 511: 0.154743\nCost after iteration 512: 0.154554\nCost after iteration 513: 0.154366\nCost after iteration 514: 0.154179\nCost after iteration 515: 0.153992\nCost after iteration 516: 0.153806\nCost after iteration 517: 0.153620\nCost after iteration 518: 0.153435\nCost after iteration 519: 0.153250\nCost after iteration 520: 0.153066\nCost after iteration 521: 0.152883\nCost after iteration 522: 0.152700\nCost after iteration 523: 0.152518\nCost after iteration 524: 0.152336\nCost after iteration 525: 0.152155\nCost after iteration 526: 0.151974\nCost after iteration 527: 0.151794\nCost after iteration 528: 0.151615\nCost after iteration 529: 0.151436\nCost after iteration 530: 0.151257\nCost after iteration 531: 0.151079\nCost after iteration 532: 0.150902\nCost after iteration 533: 0.150725\nCost after iteration 534: 0.150549\nCost after iteration 535: 0.150373\nCost after iteration 536: 0.150198\nCost after iteration 537: 0.150023\nCost after iteration 538: 0.149849\nCost after iteration 539: 0.149676\nCost after iteration 540: 0.149502\nCost after iteration 541: 0.149330\nCost after iteration 542: 0.149158\nCost after iteration 543: 0.148986\nCost after iteration 544: 0.148815\nCost after iteration 545: 0.148644\nCost after iteration 546: 0.148474\nCost after iteration 547: 0.148305\nCost after iteration 548: 0.148136\nCost after iteration 549: 0.147967\nCost after iteration 550: 0.147799\nCost after iteration 551: 0.147632\nCost after iteration 552: 0.147465\nCost after iteration 553: 0.147298\nCost after iteration 554: 0.147132\nCost after iteration 555: 0.146966\nCost after iteration 556: 0.146801\nCost after iteration 557: 0.146636\nCost after iteration 558: 0.146472\nCost after iteration 559: 0.146309\nCost after iteration 560: 0.146145\nCost after iteration 561: 0.145983\nCost after iteration 562: 0.145820\nCost after iteration 563: 0.145658\nCost after iteration 564: 0.145497\nCost after iteration 565: 0.145336\nCost after iteration 566: 0.145176\nCost after iteration 567: 0.145016\nCost after iteration 568: 0.144856\nCost after iteration 569: 0.144697\nCost after iteration 570: 0.144539\nCost after iteration 571: 0.144380\nCost after iteration 572: 0.144223\nCost after iteration 573: 0.144066\nCost after iteration 574: 0.143909\nCost after iteration 575: 0.143752\nCost after iteration 576: 0.143596\nCost after iteration 577: 0.143441\nCost after iteration 578: 0.143286\nCost after iteration 579: 0.143131\nCost after iteration 580: 0.142977\nCost after iteration 581: 0.142823\nCost after iteration 582: 0.142670\nCost after iteration 583: 0.142517\nCost after iteration 584: 0.142365\nCost after iteration 585: 0.142213\nCost after iteration 586: 0.142061\nCost after iteration 587: 0.141910\nCost after iteration 588: 0.141759\nCost after iteration 589: 0.141609\nCost after iteration 590: 0.141459\nCost after iteration 591: 0.141309\nCost after iteration 592: 0.141160\nCost after iteration 593: 0.141011\nCost after iteration 594: 0.140863\nCost after iteration 595: 0.140715\nCost after iteration 596: 0.140568\nCost after iteration 597: 0.140421\nCost after iteration 598: 0.140274\nCost after iteration 599: 0.140128\nW = [[0.49908216 0.51641999]]\nb = [[-4.69274864]]\n\n\nPlot the decision boundary:\n\nplot_decision_boundary(X_larger, Y_larger, parameters_larger)"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html",
    "title": "",
    "section": "",
    "text": "Table of contents\n- Genetic Algorithm Implementation\n- Population\n- Fitness\n- Selection\n- Crossover\n- Mutation\n- Replacement and Termination"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#genetic-algorithm-implementation",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#genetic-algorithm-implementation",
    "title": "",
    "section": "Genetic Algorithm Implementation",
    "text": "Genetic Algorithm Implementation\nSolve a 10-bit binary string optimization problem, in which you need to find the binary string with the largest total bit value."
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#population",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#population",
    "title": "",
    "section": "Population",
    "text": "Population\n\nimport numpy as np\n\n# Set a seed so that the results are consistent over sessions\nnp.random.seed(3)\n\n\ndef create_binary_population(pop_size, bit_length):\n    '''\n    Arguments:\n    pop_size -- number of individuals inside population\n    bit_length -- the number of bits of each individual (e.g. [[1010], [1101]])\n    \n    Return:\n    binary_pop -- population array of shape (pop_size, bit_length)\n    '''\n    binary_pop = np.random.randint(2, size=(pop_size, bit_length))\n    return binary_pop\n\n\npop_size, bit_length = 20, 10\nbinary_population = create_binary_population(pop_size, bit_length)\npopulation_size = len(binary_population)\nprint(\"Population:\\n\", binary_population)\n\nPopulation:\n [[0 0 1 1 0 0 0 1 1 1]\n [0 1 1 1 0 1 1 0 0 0]\n [0 1 1 0 0 0 1 0 0 0]\n [0 1 0 1 1 0 1 0 0 1]\n [1 0 0 1 0 1 0 1 1 1]\n [1 0 1 0 0 1 1 1 0 0]\n [0 1 0 0 0 1 0 0 1 1]\n [0 0 1 1 1 0 1 1 1 1]\n [1 1 0 1 0 0 1 1 0 1]\n [0 0 0 0 0 1 1 0 1 1]\n [1 0 0 1 1 0 1 0 0 0]\n [0 0 0 0 0 0 1 0 0 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 0 0 0 0 0 0 1 1 0]\n [0 0 1 0 1 1 1 0 0 1]\n [0 1 0 1 1 0 0 1 0 0]\n [1 1 1 1 1 0 0 0 0 0]\n [1 1 1 0 0 0 0 0 0 1]\n [0 1 0 0 0 1 0 1 1 1]]"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#fitness",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#fitness",
    "title": "",
    "section": "Fitness",
    "text": "Fitness\n\ndef fitness_binary_individual(individual):\n    '''\n    Return:\n    fitness -- fitness values, based on number of 1s bits (e.g. [1001] -&gt; 2)\n    '''\n    return np.sum(individual)\n\n\nfitness = np.apply_along_axis(fitness_binary_individual, 1, binary_population)\n\n\nfitness.shape\n\n(20,)\n\n\n\nprint(f'Population:\\n{binary_population}\\nFitness:\\n{fitness}')\n\nPopulation:\n[[0 0 1 1 0 0 0 1 1 1]\n [0 1 1 1 0 1 1 0 0 0]\n [0 1 1 0 0 0 1 0 0 0]\n [0 1 0 1 1 0 1 0 0 1]\n [1 0 0 1 0 1 0 1 1 1]\n [1 0 1 0 0 1 1 1 0 0]\n [0 1 0 0 0 1 0 0 1 1]\n [0 0 1 1 1 0 1 1 1 1]\n [1 1 0 1 0 0 1 1 0 1]\n [0 0 0 0 0 1 1 0 1 1]\n [1 0 0 1 1 0 1 0 0 0]\n [0 0 0 0 0 0 1 0 0 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 0 0 0 0 0 0 1 1 0]\n [0 0 1 0 1 1 1 0 0 1]\n [0 1 0 1 1 0 0 1 0 0]\n [1 1 1 1 1 0 0 0 0 0]\n [1 1 1 0 0 0 0 0 0 1]\n [0 1 0 0 0 1 0 1 1 1]]\nFitness:\n[5 5 3 5 6 5 4 7 6 4 4 1 6 6 2 5 4 5 4 5]"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#selection",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#selection",
    "title": "",
    "section": "Selection",
    "text": "Selection\nUse Roulette Wheel Selection to select parents based on their fitness. The probability of selecting an individual is proportional to its fitness.\n\ndef rouletter_wheel_selection(population, fitness):\n    '''\n    Return:\n    selected_population -- selected individuals based on fitness\n    '''\n    # fitness = fitness.ravel()\n    fitness_sum = np.sum(fitness)\n    if fitness_sum == 0:\n        raise ValueError(\"Total fitness is zero, cannot perform selection.\")\n    probabilities = fitness / fitness_sum\n    selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n    selected_population = population[selected_indices]\n    return selected_population\n\n\nselected_population = rouletter_wheel_selection(binary_population, fitness)\n\n\nprint(f'Population:\\n{binary_population}\\nFitness:\\n{fitness}\\nSelected Population:\\n{selected_population}')\n\nPopulation:\n[[0 0 1 1 0 0 0 1 1 1]\n [0 1 1 1 0 1 1 0 0 0]\n [0 1 1 0 0 0 1 0 0 0]\n [0 1 0 1 1 0 1 0 0 1]\n [1 0 0 1 0 1 0 1 1 1]\n [1 0 1 0 0 1 1 1 0 0]\n [0 1 0 0 0 1 0 0 1 1]\n [0 0 1 1 1 0 1 1 1 1]\n [1 1 0 1 0 0 1 1 0 1]\n [0 0 0 0 0 1 1 0 1 1]\n [1 0 0 1 1 0 1 0 0 0]\n [0 0 0 0 0 0 1 0 0 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 0 0 0 0 0 0 1 1 0]\n [0 0 1 0 1 1 1 0 0 1]\n [0 1 0 1 1 0 0 1 0 0]\n [1 1 1 1 1 0 0 0 0 0]\n [1 1 1 0 0 0 0 0 0 1]\n [0 1 0 0 0 1 0 1 1 1]]\nFitness:\n[5 5 3 5 6 5 4 7 6 4 4 1 6 6 2 5 4 5 4 5]\nSelected Population:\n[[0 1 0 1 1 0 1 0 0 1]\n [0 1 1 1 1 0 0 1 1 0]\n [0 0 1 0 1 1 1 0 0 1]\n [0 1 0 0 0 1 0 1 1 1]\n [1 1 1 1 1 0 0 0 0 0]\n [1 0 0 1 1 0 1 0 0 0]\n [0 1 1 1 0 1 1 0 0 0]\n [1 1 0 1 0 0 1 1 0 1]\n [1 1 1 0 0 0 0 0 0 1]\n [0 0 1 0 1 1 1 0 0 1]\n [1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 0 1 1 0 1]\n [1 0 0 1 0 1 0 1 1 1]\n [0 1 0 1 1 0 1 0 0 1]\n [0 1 1 1 0 1 1 0 0 0]\n [1 1 1 1 1 0 0 0 0 0]\n [0 1 1 1 1 0 0 1 1 0]\n [0 1 0 1 1 0 1 0 0 1]\n [0 0 1 1 1 0 1 1 1 1]\n [0 0 1 1 0 0 0 1 1 1]]"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#crossover",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#crossover",
    "title": "",
    "section": "Crossover",
    "text": "Crossover\n\ndef one_point_crossover(parent1, parent2):\n    '''\n    Perform one-point crossover between two parents.\n    \n    Arguments:\n    parent1 -- first parent binary array\n    parent2 -- second parent binary array\n    \n    Return:\n    child1 -- first child binary array\n    child2 -- second child binary array\n    '''\n    point = np.random.randint(1, len(parent1)) # random integer from low(inclusive) to high(exclusive)\n    child1 = np.concatenate((parent1[:point], parent2[point:]))\n    child2 = np.concatenate((parent2[:point], parent1[point:]))\n    return child1, child2\n\n\nparent_idx = np.array(np.random.choice(population_size, 2, replace=False))\nparent1, parent2 = selected_population[parent_idx]\nchild1, child2 = one_point_crossover(parent1, parent2)\n\n\nprint(f'Parent 1: {parent1}\\nParent 2: {parent2}\\nChild 1: {child1}\\nChild 2: {child2}')\n\nParent 1: [0 1 1 1 1 0 0 1 1 0]\nParent 2: [0 1 0 0 0 1 0 1 1 1]\nChild 1: [0 1 1 1 0 1 0 1 1 1]\nChild 2: [0 1 0 0 1 0 0 1 1 0]"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#mutation",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#mutation",
    "title": "",
    "section": "Mutation",
    "text": "Mutation\n\ndef bit_flip_mutation(individual, mutation_rate):\n    random_probs = np.random.random(individual.shape)\n    return np.where(random_probs &lt; mutation_rate, 1-individual, individual)\n\n\nmutation_rate = 0.9\n\n# child 1 mutation\nchild1_mutation = bit_flip_mutation(child1, mutation_rate)\n# print(f'Random Probs used to mutate child: {random_probs}')\nprint(f'Child1: {child1}\\nChild1 Mutation: {child1_mutation}')\n\nChild1: [0 1 1 1 0 1 0 1 1 1]\nChild1 Mutation: [1 0 0 0 1 0 1 0 0 0]"
  },
  {
    "objectID": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#replacement-and-termination",
    "href": "notebooks/optimization/genetic_algorithm/genetic_algorithm.html#replacement-and-termination",
    "title": "",
    "section": "Replacement and Termination",
    "text": "Replacement and Termination\n\nimport numpy as np\n\n# Set a seed so that the results are consistent over sessions\nnp.random.seed(3)\n\ndef create_binary_population(pop_size, bit_length):\n    '''\n    Arguments:\n    pop_size -- number of individuals inside population\n    bit_length -- the number of bits of each individual (e.g. [[1010], [1101]])\n    \n    Return:\n    binary_pop -- population array of shape (pop_size, bit_length)\n    '''\n    binary_pop = np.random.randint(2, size=(pop_size, bit_length))\n    return binary_pop\n\ndef fitness_binary_individual(individual):\n    '''\n    Return:\n    fitness -- fitness values, based on number of 1s bits (e.g. [1001] -&gt; 2)\n    '''\n    return np.sum(individual)\n\ndef rouletter_wheel_selection(population, fitness):\n    '''\n    Return:\n    selected_population -- selected individuals based on fitness\n    '''\n    # fitness = fitness.ravel()\n    fitness_sum = np.sum(fitness)\n    if fitness_sum == 0:\n        raise ValueError(\"Total fitness is zero, cannot perform selection.\")\n    probabilities = fitness / fitness_sum\n    selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n    selected_population = population[selected_indices]\n    return selected_population\n\ndef one_point_crossover(parent1, parent2):\n    '''\n    Perform one-point crossover between two parents.\n    \n    Arguments:\n    parent1 -- first parent binary array\n    parent2 -- second parent binary array\n    \n    Return:\n    child1 -- first child binary array\n    child2 -- second child binary array\n    '''\n    point = np.random.randint(1, len(parent1)) # random integer from low(inclusive) to high(exclusive)\n    child1 = np.concatenate((parent1[:point], parent2[point:]))\n    child2 = np.concatenate((parent2[:point], parent1[point:]))\n    return child1, child2\n\ndef bit_flip_mutation(individual, mutation_rate):\n    random_probs = np.random.random(individual.shape)\n    return np.where(random_probs &lt; mutation_rate, 1-individual, individual)\n\n\npopulation_size = 20\nbit_length = 20\nn_generations = 70\nmutation_rate = 0.01\n\nfitnesses = []\n\npopulation = create_binary_population(population_size, bit_length)\n\n\nfor i in range(n_generations):\n    fitness_values = np.apply_along_axis(fitness_binary_individual, 1, population)\n    fitnesses.append(np.max(fitness_values))\n    # print(f'Best: {fitnesses[-1]}')\n    \n    selected_population = rouletter_wheel_selection(population, fitness_values)\n    new_population = []\n    \n    while(len(new_population) &lt; population_size):\n        # Selection\n        # Randomly choose 2 individuals from selected population\n        parent_idx = np.array(np.random.choice(population_size, 2, replace=False))\n        parent1, parent2 = selected_population[parent_idx]\n        \n        # Crossover\n        child1, child2 = one_point_crossover(parent1, parent2)\n        \n        # Mutation\n        child1_mutation = bit_flip_mutation(child1, mutation_rate)\n        child2_mutation = bit_flip_mutation(child2, mutation_rate)\n        \n        # Add children to new population\n        new_population.append(child1_mutation)\n        new_population.append(child2_mutation)\n        \n    population = np.array(new_population[:population_size])\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(fitnesses)\nplt.xlabel('Generation')\nplt.ylabel('Fitness');"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html",
    "href": "notebooks/python_snippets/numpy_nd_array.html",
    "title": "Review Numpy basic",
    "section": "",
    "text": "Sum of elements and store in sum_of_matrix variable\nFind max/min element in matrix and store in max_value/min_value variable\nimport numpy as np\nmatrix = np.array([[1, 5, 3], [9, 2, 6], [4, 8, 7], [0, 3, 2]])\nsum_of_matrix = np.sum(matrix)\nmax_value, min_value = np.max(matrix), np.min(matrix)\nprint(sum_of_matrix, max_value, min_value)\n\n50 9 0\nmatrix = np.array([[1, 5, 3], [9, 2, 6], [4, 8, 7], [0, 3, 2]])\nmax_idx = np.unravel_index(np.argmax(matrix), matrix.shape)\nmin_idx = np.unravel_index(np.argmin(matrix), matrix.shape)\nmax_value = matrix[max_idx]\nmin_value = matrix[min_idx]\nprint(max_value, min_value)\n\n9 0"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#from-list",
    "href": "notebooks/python_snippets/numpy_nd_array.html#from-list",
    "title": "Review Numpy basic",
    "section": "From List",
    "text": "From List\n\nCreate 1D array from list [1, 2, 3, 4, 5]\nCreate 2D array of shape 2x3 from list [[1, 2, 3], [4, 5, 6]]\nCreate 3D array of shape 2x2x3 from list [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\n\n\nimport numpy as np\n\nlist_1d = np.array([1, 2, 3, 4, 5])\nlist_2d = np.array([[1, 2, 3], [4, 5, 6]])\nlist_3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nprint('list_1d: ', list_1d)\nprint('list_2d: ', list_2d)\nprint('list_3d: ', list_3d)\n\nlist_1d:  [1 2 3 4 5]\nlist_2d:  [[1 2 3]\n [4 5 6]]\nlist_3d:  [[[ 1  2  3]\n  [ 4  5  6]]\n\n [[ 7  8  9]\n  [10 11 12]]]\n\n\n\nlist_3d.dtype, list_3d.shape, list_3d.ndim\n\n(dtype('int64'), (2, 2, 3), 3)"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.zerosshape-dtypefloat-orderc-likenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.zerosshape-dtypefloat-orderc-likenone",
    "title": "Review Numpy basic",
    "section": "np.zeros(shape, dtype=float, order='C', *, like=None)",
    "text": "np.zeros(shape, dtype=float, order='C', *, like=None)\n\nimport numpy as np\n\nzeros_matrix_2x3 = np.zeros((2,3))\nprint(zeros_matrix_2x3)\n\n[[0. 0. 0.]\n [0. 0. 0.]]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.onesshape-dtypenone-orderc-likenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.onesshape-dtypenone-orderc-likenone",
    "title": "Review Numpy basic",
    "section": "np.ones(shape, dtype=None, order='C', *, like=None)",
    "text": "np.ones(shape, dtype=None, order='C', *, like=None)\n\nimport numpy as np\n\nones_matrix_2x3 = np.ones((2,3))\nprint(ones_matrix_2x3)\n\n[[1. 1. 1.]\n [1. 1. 1.]]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.fullshape-fill_value-dtypenone-orderc-likenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.fullshape-fill_value-dtypenone-orderc-likenone",
    "title": "Review Numpy basic",
    "section": "np.full(shape, fill_value, dtype=None, order='C', *, like=None)",
    "text": "np.full(shape, fill_value, dtype=None, order='C', *, like=None)\n\nimport numpy as np\n\nfull_of_9_matrix = np.full((2,3), 9)\nprint(full_of_9_matrix)\n\n[[9 9 9]\n [9 9 9]]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.arangestart-stop-step-dtypenone-likenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.arangestart-stop-step-dtypenone-likenone",
    "title": "Review Numpy basic",
    "section": "np.arange([start, ]stop, [step, ]dtype=None, *, like=None)",
    "text": "np.arange([start, ]stop, [step, ]dtype=None, *, like=None)\n\nimport numpy as np\n\narange_numpy = np.arange(start=0, stop=5, step=1)\nprint(arange_numpy)\n\n[0 1 2 3 4]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.eyen-mnone-k0-dtypeclass-float-orderc-likenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.eyen-mnone-k0-dtypeclass-float-orderc-likenone",
    "title": "Review Numpy basic",
    "section": "np.eye(N, M=None, k=0, dtype=<class 'float'>, order='C', *, like=None)",
    "text": "np.eye(N, M=None, k=0, dtype=&lt;class 'float'&gt;, order='C', *, like=None)\n\nimport numpy as np\n\nones_in_diagonal_zeros_elsewhere = np.eye(3)\nprint(ones_in_diagonal_zeros_elsewhere)\n\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.random.randomsizenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.random.randomsizenone",
    "title": "Review Numpy basic",
    "section": "np.random.random(size=None)",
    "text": "np.random.random(size=None)\nReturn random floats in the half-open interval [0.0, 1.0).\n\nimport numpy as np\n\nmy_arr = np.random.random((2,3))\nprint(my_arr)\n\n[[0.3511155  0.15105555 0.04231165]\n [0.38675743 0.85587757 0.31778496]]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.clipa-a_min-a_max-outnone-kwargs",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.clipa-a_min-a_max-outnone-kwargs",
    "title": "Review Numpy basic",
    "section": "np.clip(a, a_min, a_max, out=None, **kwargs)",
    "text": "np.clip(a, a_min, a_max, out=None, **kwargs)\nAny values less than a_min are set to a_min, and any values greater than a_max are set to a_max.\n\nimport numpy as np\n\narr = np.array([1,5,8,10,3,6])\nclipped_arr = np.clip(arr, a_min=3, a_max=8)\nprint(clipped_arr)\n\n[3 5 8 8 3 6]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.concatenatea1-a2-...-axis0-outnone-dtypenone-castingsame_kind",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.concatenatea1-a2-...-axis0-outnone-dtypenone-castingsame_kind",
    "title": "Review Numpy basic",
    "section": "np.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")",
    "text": "np.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n\nimport numpy as np\n\narr1 = np.array([1,2,3])\narr2 = np.array([4,5,6])\nconcatenated = np.concatenate((arr1, arr2))\nprint(concatenated)\n\n[1 2 3 4 5 6]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#add-more-axes-to-the-array",
    "href": "notebooks/python_snippets/numpy_nd_array.html#add-more-axes-to-the-array",
    "title": "Review Numpy basic",
    "section": "Add more axes to the array",
    "text": "Add more axes to the array\n\nimport numpy as np\n\narr = np.array([1,2,3,4,5])\nnew_arr1 = arr[:, np.newaxis]\nnew_arr2 = arr[np.newaxis, :]\nprint(new_arr1.shape, new_arr1)\nprint(new_arr2.shape, new_arr2)\n\n(5, 1) [[1]\n [2]\n [3]\n [4]\n [5]]\n(1, 5) [[1 2 3 4 5]]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.vectorizepyfunc-otypesnone-docnone-excludednone-cachefalse-signaturenone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.vectorizepyfunc-otypesnone-docnone-excludednone-cachefalse-signaturenone",
    "title": "Review Numpy basic",
    "section": "np.vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False, signature=None)",
    "text": "np.vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False, signature=None)\nThis function converts a Python function that operates on scalars into a vectorized function that can operate on NumPy arrays.\n\nimport numpy as np\n\ndef calculate_circle_area(radius):\n    return np.pi * (radius**2)\n\nradius_values = np.array([1,2,3,4])\nvectorized_func = np.vectorize(calculate_circle_area)\narea_values = vectorized_func(radius_values)\nprint(area_values)\n\n[ 3.14159265 12.56637061 28.27433388 50.26548246]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#np.wherecondition-xnone-ynone-outnone",
    "href": "notebooks/python_snippets/numpy_nd_array.html#np.wherecondition-xnone-ynone-outnone",
    "title": "Review Numpy basic",
    "section": "np.where(condition, x=None, y=None, *, out=None)",
    "text": "np.where(condition, x=None, y=None, *, out=None)\n\nimport numpy as np\n\narr = np.array([1,2,3,4,5])\ngreater_than_3 = np.where(arr &gt; 3, 0, arr)\nprint(greater_than_3)\n\n[1 2 3 0 0]"
  },
  {
    "objectID": "notebooks/python_snippets/numpy_nd_array.html#array.flattenorderc",
    "href": "notebooks/python_snippets/numpy_nd_array.html#array.flattenorderc",
    "title": "Review Numpy basic",
    "section": "<array>.flatten(order='C')",
    "text": "&lt;array&gt;.flatten(order='C')\n\nimport numpy as np\n\narr = np.array([[1, 2, 3], [4, 5, 6]])\nflattened_arr = arr.flatten()"
  },
  {
    "objectID": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html",
    "href": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html",
    "title": "Python 2D List",
    "section": "",
    "text": "def diagonal_sum(matrix):\n    # Check if the matrix is square\n    if not matrix or len(matrix) != len(matrix[0]):\n        raise ValueError(\"The matrix must be square.\")\n    # Calculate the sum of the main diagonal\n    return sum([matrix[i][i] for i in range(len(matrix))])\n\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(diagonal_sum(matrix))  # Output: 15\n\n15\n\n\n\n\n\n\ndef sort_matrix_by_row_sum(matrix):\n    return sorted(matrix, key=lambda row: sum(row))\n\n\nmatrix = [[4, 2, 7], [1, 5, 6], [3, 8, 2]]\nprint(sort_matrix_by_row_sum(matrix))\n\n[[1, 5, 6], [4, 2, 7], [3, 8, 2]]\n\n\n\n\n\nA company has a list of inventory items for various products. The inventory information includes the product name, quantity in stock, and the selling price of each product. The company wants to calculate the total value of its inventory.\nThe company’s inventory list is represented by a matrix of size nx3, where n is the number of different products. The columns in the matrix represent the product name, quantity in stock, and selling price of each product.\nFor example, the inventory matrix might look like this: Inventory:\n|    Name    | Quantity | Price |\n|  Product 1 |    10    |   5.0   |\n|  Product 2 |    5     |   2.5   |\n|  Product 3 |    3     |   8.0   |\n\nCalculate the total quantity of inventory for all products.\nCalculate the inventory value of each product by multiplying the quantity by the selling price of the corresponding product.\nCalculate the total value of the entire inventory.\n\nWrite equations or code to solve each of the above exercises.\n\n# Inventory matrix\ninventory = [[\"Product 1\", 10, 5.0], [\"Product 2\", 5, 2.5], [\"Product 3\", 3, 8.0]]\n\n# Calculate the total quantity of inventory for all products\ntotal_quantity = sum(item[1] for item in inventory)\nprint(\"Total inventory quantity:\", total_quantity)\n\n# Calculate the inventory value of each product\nproduct_values = [item[1] * item[2] for item in inventory]\nprint(\"Inventory value of each product:\", product_values)\n\n# Calculate the total value of the entire inventory\ntotal_value = sum(product_values)\nprint(\"Total inventory value:\", total_value)\n\nTotal inventory quantity: 18\nInventory value of each product: [50.0, 12.5, 24.0]\nTotal inventory value: 86.5"
  },
  {
    "objectID": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#basic",
    "href": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#basic",
    "title": "Python 2D List",
    "section": "",
    "text": "def diagonal_sum(matrix):\n    # Check if the matrix is square\n    if not matrix or len(matrix) != len(matrix[0]):\n        raise ValueError(\"The matrix must be square.\")\n    # Calculate the sum of the main diagonal\n    return sum([matrix[i][i] for i in range(len(matrix))])\n\n\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(diagonal_sum(matrix))  # Output: 15\n\n15\n\n\n\n\n\n\ndef sort_matrix_by_row_sum(matrix):\n    return sorted(matrix, key=lambda row: sum(row))\n\n\nmatrix = [[4, 2, 7], [1, 5, 6], [3, 8, 2]]\nprint(sort_matrix_by_row_sum(matrix))\n\n[[1, 5, 6], [4, 2, 7], [3, 8, 2]]\n\n\n\n\n\nA company has a list of inventory items for various products. The inventory information includes the product name, quantity in stock, and the selling price of each product. The company wants to calculate the total value of its inventory.\nThe company’s inventory list is represented by a matrix of size nx3, where n is the number of different products. The columns in the matrix represent the product name, quantity in stock, and selling price of each product.\nFor example, the inventory matrix might look like this: Inventory:\n|    Name    | Quantity | Price |\n|  Product 1 |    10    |   5.0   |\n|  Product 2 |    5     |   2.5   |\n|  Product 3 |    3     |   8.0   |\n\nCalculate the total quantity of inventory for all products.\nCalculate the inventory value of each product by multiplying the quantity by the selling price of the corresponding product.\nCalculate the total value of the entire inventory.\n\nWrite equations or code to solve each of the above exercises.\n\n# Inventory matrix\ninventory = [[\"Product 1\", 10, 5.0], [\"Product 2\", 5, 2.5], [\"Product 3\", 3, 8.0]]\n\n# Calculate the total quantity of inventory for all products\ntotal_quantity = sum(item[1] for item in inventory)\nprint(\"Total inventory quantity:\", total_quantity)\n\n# Calculate the inventory value of each product\nproduct_values = [item[1] * item[2] for item in inventory]\nprint(\"Inventory value of each product:\", product_values)\n\n# Calculate the total value of the entire inventory\ntotal_value = sum(product_values)\nprint(\"Total inventory value:\", total_value)\n\nTotal inventory quantity: 18\nInventory value of each product: [50.0, 12.5, 24.0]\nTotal inventory value: 86.5"
  },
  {
    "objectID": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#basic-1",
    "href": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#basic-1",
    "title": "Python 2D List",
    "section": "Basic",
    "text": "Basic\n\nExercise 1: Find maximum value in a 3D list\n\ndef find_max(list_3d):\n    return max(max(list_1d) for list_2d in list_3d for list_1d in list_2d if list_1d)\n\n\nlist_3d = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\nprint(find_max(list_3d))\n\n12\n\n\n\n\nExercise 2: Flatten a 3D list into a 1D list\n\nSolution 1: Using Loops\n\n# Use 3 nested loops to flatten the 3D list\ndef flatten_3d_list(list_3d):\n    # Initialize an empty list to hold the flattened elements\n    flat_list = []\n    # Iterate through each 2D list in the 3D list\n    for list_2d in list_3d:\n        # Iterate through each 1D list in the 2D list\n        for list_1d in list_2d:\n            # Iterate through each item in the 1D list and append it to the flat_list\n            for item in list_1d:\n                flat_list.append(item)\n    return flat_list\n\n\nlist_3d = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\nprint(flatten_3d_list(list_3d))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\n\n\n\nSolution 2: Using List Comprehension\n\ndef flatten_3d_list(list_3d):\n    return [item for list_2d in list_3d for list_1d in list_2d for item in list_1d]\n\n\nlist_3d = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\nprint(flatten_3d_list(list_3d))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\n\n\n\n\nExercise 3: Rotate a 3D list (matrix) 90 degrees clockwise\n\ndef rotate_3d_clockwise(list_3d):\n    # zip(*list_3d) transposes the 3D list, and [list(reversed(x)) for x in ...] reverses each sublist to achieve a 90-degree clockwise rotation\n    # use * to unpack the list_3d for zip function, which groups elements from each sublist together\n    # if not using *, zip would treat the entire list_3d as a single argument, resulting in incorrect grouping\n    return [list(reversed(x)) for x in zip(*list_3d)]\n\n\nlist_3d = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\nprint(rotate_3d_clockwise(list_3d))\n\n[[[7, 8, 9], [1, 2, 3]], [[10, 11, 12], [4, 5, 6]]]"
  },
  {
    "objectID": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#numpy-1d-list",
    "href": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#numpy-1d-list",
    "title": "Python 2D List",
    "section": "Numpy 1D List",
    "text": "Numpy 1D List\n\nExercise 1: Create 5 elements array\n\nmy_arr = np.array([1, 2, 3, 4, 5])\nprint(my_arr)\n\n[1 2 3 4 5]\n\n\n\n\nExercise 2: Sum of all elements in a 1D array\n\nmy_arr = np.array([1, 2, 3, 4, 5])\nsum = np.sum(my_arr)\nprint(sum)\n\n15\n\n\n\n\nExercise 3: Min/Max\n\nmy_arr = np.array([1, 2, 3, 4, 5])\nmin_value, max_value = np.min(my_arr), np.max(my_arr)\nprint(min_value, max_value)\n\n1 5\n\n\n\n\nExercise 4: Mean/Median of a 1D array\n\nmy_arr = np.array([1, 2, 3, 4, 5])\nmedian = np.mean(my_arr)\nprint(median)\n\n3.0\n\n\n\n\nExercise 5: Sort ascending/descending order a 1D array\n\nSolution 1: Using np.sort() and slicing\n\nmy_arr = np.array([1, 2, 3, 4, 5])\nascending = np.sort(my_arr)\ndescending = ascending[::-1]\nprint(ascending, descending)\n\n[1 2 3 4 5] [5 4 3 2 1]\n\n\n\n\nSolution 2: Using np.sort() and np.flip()\n\ndescending = np.flip(ascending)\nprint(ascending, descending)\n\n[1 2 3 4 5] [5 4 3 2 1]\n\n\n\n\n\nExercise 6: Sum of elements at odd/even indices in a 1D array\n\nmy_arr = np.array([1, 2, 3, 4, 5])\nsum_odd_indices = np.sum(my_arr[::2])\nsum_even_indices = np.sum(my_arr[1::2])\nprint(sum_odd_indices, sum_even_indices)\n\n9 6\n\n\n\n\nExercise 7: Find even numbers are their summation in a 1D array\n\nmy_arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\neven = my_arr[my_arr % 2 == 0]\neven_sum = np.sum(even)\nprint(even_sum, even)\n\n30 [ 2  4  6  8 10]\n\n\n\n\nExercise 8: Sum of prime numbers in a 1D array\nnp.vectorize()\n\nimport math\n\n\ndef is_prime(n):\n    if n &lt; 2:\n        return False\n    for i in range(2, int(math.sqrt(n)) + 1):\n        if n % i == 0:\n            return False\n    return True\n\n\nmy_arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nprimes = my_arr[np.vectorize(is_prime)(my_arr)]\nprimes_sum = np.sum(primes)\nprint(primes, primes_sum)\n\n[2 3 5 7] 17\n\n\n\n\nExercise 9: Sum of positive and product of negative numbers in a 1D array\nnp.sum(), np.prod()\n\nmy_arr = np.array([-1, 2, -3, 4, -5, 6])\npositive_sum = np.sum(my_arr[my_arr &gt; 0])\nnegative_sum = np.prod(my_arr[my_arr &lt; 0])\nprint(positive_sum, negative_sum)\n\n12 -15\n\n\n\n\nExercise 10: Sum of elements after removing duplicates in a 1D array\nnp.unique()\n\nmy_array = [1, 2, 3, 2, 4, 5, 1, 6, 7, 8, 5]\nunique = np.unique(my_array)\nunique_sum = np.sum(unique)\nprint(unique, unique_sum)\n\n[1 2 3 4 5 6 7 8] 36"
  },
  {
    "objectID": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#numpy-2d-list",
    "href": "notebooks/python_snippets/python_numpy_1d_2d_3d_list.html#numpy-2d-list",
    "title": "Python 2D List",
    "section": "Numpy 2D List",
    "text": "Numpy 2D List\n\nBasic\n\nExercise 1: Sum of elements in a 2D array\n\nmy_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(np.sum(my_array))\n\n45\n\n\n\n\nExercise 2: Min/Max in a 2D array\n\nmy_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(np.min(my_array), np.max(my_array))\n\n1 9\n\n\n\n\nExercise 3: Mean/Median in a 2D array\n\nmy_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(np.mean(my_array))\n\n5.0\n\n\n\n\nExercise 4: Sort rows in a 2D array\n\nmy_array = [[5, 3, 1], [4, 2, 6], [9, 7, 8]]\nascending = np.sort(my_array, axis=1)\ndescending = np.flip(ascending)\nprint(ascending)\nprint(descending)\n\n[[1 3 5]\n [2 4 6]\n [7 8 9]]\n[[9 8 7]\n [6 4 2]\n [5 3 1]]\n\n\n\n\nExercise 5: Sum of each row and sum of rows sum in a 2D array\n\nmy_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nrows_sum = np.sum(my_array, axis=1)\nrows_sum_sum = np.sum(rows_sum)\nprint(rows_sum, rows_sum_sum)\n\n[ 6 15 24] 45\n\n\n\n\nExercise 6: Sum of each column and sum of columns sum in a 2D array\n\nmy_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ncolumns_sum = np.sum(my_array, axis=0)\ntotal_sum = np.sum(columns_sum)\nprint(columns_sum, total_sum)\n\n[12 15 18] 45\n\n\n\n\nExercise 7: Find max and its position in a 2D array\n\n# mp.unravel_index(np.argmax(my_array), my_array.shape) to get the 2D index of the max value\nmy_array = np.array([[1, 2, 3], [4, 9, 6], [7, 8, 5]])\nmax_value = np.max(my_array)\nmax_value_index = np.unravel_index(np.argmax(my_array), my_array.shape)\nprint(max_value, max_value_index)\n\n9 (np.int64(1), np.int64(1))"
  }
]